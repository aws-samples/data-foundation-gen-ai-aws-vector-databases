{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb05169",
   "metadata": {},
   "source": [
    "# Text Chunking (optional lab)\n",
    "\n",
    "This optional lab will walk you through various methods to perform chunking of text in your documents. Retrieval is a very important step in RAG architecture. Semantic search requires you take your knowledge/text and convert that into embeddings and store them in a search engine that offers vector search capability. To convert your documents into embedding, you will need to split them into smaller pieces, popularly called \"Chunks\". This technique is known as \"Chunking\". Chunking is necessary because a large text passage may lose its specificity, it may conflate different topics, or concepts, making it not a top match for a query about a topic. This would mean even if there is a very relevant information in one part of a large text passage, the similarity of the text passage as a whole to user's query may be very low. This may exclude the text passage from top semantic search results. Remember we only use few top results in our prompt to LLM that generates final text for answer.\n",
    "\n",
    "\n",
    "There is a [great resource by Greg Kamradt](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) from where you can learn about various ways to chunk text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fb6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community pypdf langchain_experimental --quiet\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install --upgrade --quiet  boto3\n",
    "!pip install opensearch-py --quiet\n",
    "\n",
    "#You can safely ignore the version requirement error for opensearchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1915eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87760ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "#a client to bedrock runtime.\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86734",
   "metadata": {},
   "source": [
    "# Recursive character chunking\n",
    "The most simplist way to chunk document would be by length, but keeping paragraphs or lines together so it does not lose the meaning. We will use lang chain library that provides a recurisve character text splitter which offers ways to split data by length, yet keeps the lines, paragraph together as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a82f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method would split the text into chunks by paragraph, line boundary and keeping chunk \n",
    "# size as close to 1000 characters, it will also overlap the text between chunks if it were to \n",
    "# split line or paragraph in the middle.\n",
    "\n",
    "def recursive_character_chunking(text): \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        #separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=100, #number of characters that can overlap with previous chunk\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.create_documents(text)#From the loaded PDF\n",
    "    \n",
    "    return docs #return the index to be cached by the client app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b894d",
   "metadata": {},
   "source": [
    "Let's try to run this method on an excerpt of AWS docs from Amazon Bedrock titan model and Amazon Textract services. You will notice that length/recursive chunking will create chunks with overlaps, this helps in situations where sentences need to not be chopped in the middle, but it will fail to keep Textract and Titan documentation chunks separate. You will notice that chunk no. 9 is a mix of titan and lambda docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "# lets load text from our prepared aws-docs-excerpt from various services.\n",
    "# this document contains only sections of docs across multiple services.\n",
    "\n",
    "with open('aws-docs-excerpt.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "docs = recursive_character_chunking([text])\n",
    "\n",
    "# the method prints chunks\n",
    "def print_chunks(data):\n",
    "    # Let's print the chunks -- notice the overlap between chunk 6 and 7\n",
    "    # However, mostly it is a clean separation at the end of the sentence.\n",
    "    i = 1\n",
    "    for doc in data:\n",
    "        print(f\"---------START OF CHUNK {i}------\")\n",
    "        print(f\"{doc.page_content}\")\n",
    "        print(f\"---------END OF CHUNK {i}------\\n\\n\")\n",
    "        i+=1\n",
    "        \n",
    "print_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9926627",
   "metadata": {},
   "source": [
    "### Connect to Amazon OpenSearch Service\n",
    "Following cell makes a connection with opensearch, we will use `aos_client` throughout this lab. If you get security token expiration error, please run this cell again and authentication will be refreshed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "# For this lab we will use credentials that we have already created in AWS Secrets manager service. Secrets\n",
    "# manager service allows you to store secrets securily and retrieve it through code in a safe manner.\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47598d07",
   "metadata": {},
   "source": [
    "## Loading text chunks in opensearch to run semantic search over chunks\n",
    "\n",
    "#### Let's first create a few helper methods to work with vector indices and embeddings.\n",
    "\n",
    "`embed_phrase` - this method call Amazon bedrock API and converts a text into embedding.\n",
    "\n",
    "`get_model_dimension` - this helper method takes a model id and returns its dimensions. \n",
    "\n",
    "`create_opensearch_vector_index` - this method creates a vector index in opensearch. It uses model id to determine the dimension of the vector field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method would create embedding from a given text.\n",
    "def embed_phrase( text, model_id, bedrock_client ):\n",
    "    model_id = model_id # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Extract the embedding from the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # Append the embedding to the list\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "def get_model_dimension(model_id):\n",
    "    if model_id==\"amazon.titan-embed-text-v2:0\":\n",
    "        return 1024\n",
    "    if model_id.startswith(\"cohere\"):\n",
    "        return 512\n",
    "    if model_id.startswith(\"amazon.titan-embed-text-v1\"):\n",
    "        return 8192\n",
    "    if model_id.startswith(\"amazon.titan-embed-image-v1\"):\n",
    "        return 8192\n",
    "    \n",
    "def create_opensearch_vector_index(index_name, model_id):    \n",
    "    knn_index = {\n",
    "        \"settings\": {\n",
    "            \"index.knn\": True,\n",
    "            \"index.knn.space_type\": \"cosinesimil\",\n",
    "            \"analysis\": {\n",
    "              \"analyzer\": {\n",
    "                \"default\": {\n",
    "                  \"type\": \"standard\",\n",
    "                  \"stopwords\": \"_english_\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"chunk_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    #we will set dimension based on selected model\n",
    "                    \"dimension\": get_model_dimension(model_id=model_id), \n",
    "                    \"store\": True\n",
    "                },\n",
    "                \"chunk_content\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"store\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        aos_client.indices.delete(index=index_name)\n",
    "        print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "        aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "    except:\n",
    "        print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "        aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1128e08",
   "metadata": {},
   "source": [
    "### Create a vector index in opensearch\n",
    "Following code creates an index for our knowledgebase. It uses previously created `create_opensearch_vector_index` python method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71459ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_recursive_chunk = \"aws_docs_recursive_chunk_index\"\n",
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "create_opensearch_vector_index(index_name_recursive_chunk, model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2edb30",
   "metadata": {},
   "source": [
    "### Creating embeddings\n",
    "Following cell shows a call to `embed_phrase` method that we created earlier. This method would create an embedding for a given model. In this case `amazon.titan-embed-text-v2:0`. We print first 5 floating point numbers from array of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b809432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test calling embed_phrase method from utilities file to get embedding of a given model.\n",
    "embedding = embed_phrase(\"Testing amazon bedrock models\", model_id, bedrock_client=bedrock_client)\n",
    "embedding[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72945a",
   "metadata": {},
   "source": [
    "### bulk load data in to opensearch\n",
    "Following cell define a method that take text chunks and loads them into opensearch using a bulk load method. We will print our progress as we load data using `alive_bar` and `bar()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process all the chunks and get embeddings from Bedrock for each text chunk\n",
    "def bulk_load_chunks_in_opensearch(docs, index_name, model_id, bedrock_client):\n",
    "    chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        chunks.append({\n",
    "            #following field is the actual text of the chunk\n",
    "            \"chunk_content\": doc.page_content, \n",
    "         \n",
    "            #following json field will contain vector embedding\n",
    "            \"chunk_vector\": embed_phrase(doc.page_content, model_id, bedrock_client)\n",
    "        })\n",
    "\n",
    "    #load data into opensearch - every chunk will be separate opensearch record/document.\n",
    "    cnt = 0\n",
    "    batch = 0\n",
    "    action = json.dumps({ \"index\": { \"_index\": index_name } })\n",
    "    body_ = ''\n",
    "\n",
    "    print(f\"loading {len(docs)} chunks\")\n",
    "\n",
    "    with alive_bar(len(docs), force_tty = True) as bar:\n",
    "        for doc in chunks:\n",
    "\n",
    "            payload=doc #each doc is json document\n",
    "            body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "            cnt = cnt+1\n",
    "\n",
    "            if(cnt == 100):\n",
    "\n",
    "                response = aos_client.bulk(\n",
    "                                    index = index_name,\n",
    "                                     body = body_)\n",
    "\n",
    "\n",
    "                cnt = 0\n",
    "                batch = batch +1\n",
    "                body_ = \"\"\n",
    "                print(\"Total Bulk batches completed: \"+str(batch))\n",
    "\n",
    "            bar()\n",
    "\n",
    "\n",
    "        #process last batch\n",
    "        if body_ != \"\":\n",
    "            response = aos_client.bulk(\n",
    "                                    index = index_name,\n",
    "                                     body = body_)\n",
    "\n",
    "\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "            print(\"Total Bulk batches completed: \"+str(batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9ed88",
   "metadata": {},
   "source": [
    "Now lets call above method to bulk load our recursive character chunks into opensearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_load_chunks_in_opensearch(docs, index_name_recursive_chunk, model_id, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e797b",
   "metadata": {},
   "source": [
    "### Search data using vector search (semantic search)\n",
    "Following method runs a semantic search over our index and returns top N items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd633d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, index_name, model_id, bedrock_client, n=3 ):\n",
    "    search_vector = embed_phrase(phrase, model_id=model_id, bedrock_client=bedrock_client)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"chunk_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"chunk_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"chunk_content\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"chunk_content\":entry['_source']['chunk_content'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8c40f",
   "metadata": {},
   "source": [
    "### Run few sample semantic searches\n",
    "You may change the question below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs=\"What VPN connections do in a VPC?\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(phrase=question_on_docs, index_name=index_name_recursive_chunk, model_id=model_id, bedrock_client=bedrock_client, n=2)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3cc9d",
   "metadata": {},
   "source": [
    "# Semantic chunking\n",
    "Semantic chunking is a novel technique that chunks the data in a way that it optimises it for semantic cohesion. The method uses an embedding model and runs similarity calculation over sentences and decides the chunk position based on deviation/change in semantic distance between sentences. It uses rolling window where it keeps adding sentences and measure its distance with incoming sentence. Technically a change in topic should be detected (not very accurately). A breakpoint threshold is statistical method use to determine this change. This way you ensure that chunks stay optimal for semantic matching. \n",
    "\n",
    "If you are keen to get more info, read level 4 in [Greg Kamradt tutorial](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).\n",
    "\n",
    "Lang chain offers semantic chunking and also ability to call embedding model. We will first choose an embedding model for our semantic chunking a a breakpoint threshold type. After selecting the model and threshold, please move to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67023018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets initialize the code for drop down box input.\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "\n",
    "#defaults\n",
    "model_id='amazon.titan-embed-text-v2:0'\n",
    "threshold = 'percentile'\n",
    "\n",
    "#list of embedding models in bedrock\n",
    "model_list=['cohere.embed-english-v3','cohere.embed-multilingual-v3',\n",
    "            'amazon.titan-embed-text-v1','amazon.titan-embed-text-v2:0',\n",
    "           'amazon.titan-embed-image-v1']\n",
    "\n",
    "#semantic chunking \n",
    "threshold_list=['percentile', 'standard_deviation', 'interquartile']\n",
    "    \n",
    "drop1 = widgets.Dropdown(options=model_list, value='cohere.embed-english-v3', description='Model:', disabled=False)\n",
    "drop2 = widgets.Dropdown(options=threshold_list, value='percentile', description='Threshold:', disabled=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a066d49",
   "metadata": {},
   "source": [
    "Following code runs semantic chunking for aws docs text. It also shows a drop down for you to change the model and threshold type so you can see the effects of various models and breakpoint thresholds. \n",
    "\n",
    "You can select a specific model and threshold and run through this lab. You can come back to this cell and repeat this process with a different semantic chunking strategy. This will give you a good indication of how semantic chunking works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "#from langchain_community.embeddings import BedrockEmbedding\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "\n",
    "semantic_chunks = []\n",
    "\n",
    "#method that is called when drop down boxes are shown or changed\n",
    "def update_dropdown(selected_model, selected_threshold):\n",
    "    model_id = drop1.value.lower()\n",
    "    threshold = drop2.value.lower()\n",
    "    info = f\"Selected embedding model: {model_id}. Selected threshold: {threshold}!\"\n",
    "    display(info)\n",
    "    semantic_chunks = perform_semantic_chunking(text=text, model_id=model_id, threshold=threshold)\n",
    "    print_chunks(semantic_chunks)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# method runs semantic chunking on text for a given model and threshold.    \n",
    "def perform_semantic_chunking(text, model_id, threshold):\n",
    "    print(f\"Chunking using {model_id} and {threshold} threshold breaking point\")\n",
    "    \n",
    "    #using lang chain's Bedrock embedding object\n",
    "    embeddings = BedrockEmbeddings(region_name=region, model_id=model_id)\n",
    "\n",
    "    #using lang chain's semantic chunker to chunk\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings, breakpoint_threshold_type= threshold\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    semantic_chunks.clear()\n",
    "    semantic_chunks.extend(docs)\n",
    "    return docs\n",
    "\n",
    "#lets run semantic chunking and display the drop down. \n",
    "w = interactive(update_dropdown, selected_model=drop1, selected_threshold=drop2) \n",
    "display(w)\n",
    "\n",
    "\n",
    "\n",
    "#when you change value - it take around 15-20 seconds for refreshing the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e272b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae5ea4",
   "metadata": {},
   "source": [
    "You can select a different combination of the embedding model and threshold ids to see what breaks the content best. You will find results vary from one model to another and between various breakpoint threshold technique. However, it does not mean this combination will always be best for chunking. Note that this also does not mean it is optimal for retrieval. We will have to test this with our queries to know if this is best to answer our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a0e8a",
   "metadata": {},
   "source": [
    "### Loading semantic chunks in opensearch\n",
    "\n",
    "#### Let's first create an index with KNN field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunk_index_name = \"aws_docs_semantic_chunk_index\"\n",
    "create_opensearch_vector_index(semantic_chunk_index_name, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test calling embed_phrase method from utilities file to get embedding from the selected model.\n",
    "embedding = embed_phrase(\"Testing amazon bedrock models\", model_id, bedrock_client=bedrock_client)\n",
    "embedding[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ff02b",
   "metadata": {},
   "source": [
    "### bulk load data in to opensearch\n",
    "Following cell will take these semantic chunks and loads them into opensearch using a bulk load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceebb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_load_chunks_in_opensearch(semantic_chunks, semantic_chunk_index_name, model_id, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d900416",
   "metadata": {},
   "source": [
    "### Let's run vector searches on semantic chunks\n",
    "Following method runs vector search on sementically chunked data. You may change the questions to see difference in output from before.\n",
    "\n",
    "You will notice that for question - `What VPN connections do in VPC` - we have a much better result when we do semantic chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs=\"What VPN connections do in a VPC?\"\n",
    "\n",
    "example_request = retrieve_opensearch_with_semantic_search(phrase=question_on_docs, index_name=semantic_chunk_index_name, model_id=model_id, bedrock_client=bedrock_client, n=2)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435c373",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "Although semantic chunking naturally keeps chunk large size. you can see it keeps most VPC documentation in a single chunk. This makes semantic chunking better fit for this text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa317db",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs=\"What VPN connections do ?\"\n",
    "\n",
    "print(\"RECURISVE CHUNKING SEARCH RESULTS..\")\n",
    "example_request1 = retrieve_opensearch_with_semantic_search(phrase=question_on_docs, index_name=index_name_recursive_chunk, model_id=model_id, bedrock_client=bedrock_client, n=2)\n",
    "print(json.dumps(example_request1, indent=4))\n",
    "\n",
    "\n",
    "print(\"SEMANTIC CHUNKING SEARCH RESULTS..\")\n",
    "example_request2 = retrieve_opensearch_with_semantic_search(phrase=question_on_docs, index_name=semantic_chunk_index_name, model_id=model_id, bedrock_client=bedrock_client, n=2)\n",
    "print(json.dumps(example_request2, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf4dd3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lab we learned how to break text into chunks. This is a very useful skill in RAG architecture. Note that these are just 2 of many ways you can chunk the data. Chunking has a great impact on the quality of your RAG application. Learning different ways to chunk can be a handy skill to acquire.\n",
    "\n",
    "Lab is finished now. You may go to lab instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
