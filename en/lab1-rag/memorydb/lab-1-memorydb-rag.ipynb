{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# RAG using MemoryDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "[Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. \n",
    "\n",
    "In this lab we are going to be writing a simple RAG application code that allows user to ask questions about various wines so they can make a purchasing decision. We will use the vector search capability with MemoryDB to retrieve the best matching wine reviews and provide that to LLM for answering user's questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "#### a. Download and install python dependencies\n",
    "\n",
    "For this notebook we require the use of a few libraries. We'll use the Python clients for Redis and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd01d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redis pandas numpy sentence-transformers boto3 ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 3. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import boto3\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459fff5",
   "metadata": {},
   "source": [
    "#### Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a cloudformation stack in the account. Information of these resources will be used within this lab. We are going to load some of the information variables here.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "memorydb_endpoint = outputs['MemoryDBClusterEndpoint']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "## 3. Prepare data\n",
    "Below is the code that loads dataset of wine reviews, we'll use this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.json\"\n",
    "df = pd.read_json(url)\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "## 4. Create a connection with MemoryDB cluster.\n",
    "Next, we'll use Python API to set up connection with MemoryDB Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to Redis\n",
    "redis_conn = redis.RedisCluster(\n",
    "    host=memorydb_endpoint,\n",
    "    port=6379,\n",
    "    ssl=True,\n",
    "    decode_responses=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe3d",
   "metadata": {},
   "source": [
    "## 5. Using Amazon Bedrock titan embedding to convert text to vectors\n",
    "Amazon Bedrock offers Amazon titan embedding v2 model that generates vector embeddings for text. This model will be used as our primary model for embeddings.\n",
    "\n",
    "#### Helper method to invoke Titan embedding model in Amazon Bedrock\n",
    "Creating a helper method in python to invoke Amazon Titan embedding model from Amazon Bedrock to generate embeddings. We will update `df_sample` data frame and add a new column called `embedding` in it. Once this cell is executed, our data frame will be ready to load into MemoryDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{region}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def add_embeddings_to_df(df, text_column):\n",
    "\n",
    "    # Create an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over the text in the specified column\n",
    "    for text in df[text_column]:\n",
    "        embedding = embed_phrase(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_phrase( text ):\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"  # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Extract the embedding from the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "\n",
    "    # Append the embedding to the list\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "df_sample = add_embeddings_to_df(df_sample, 'description')\n",
    "\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875df0b",
   "metadata": {},
   "source": [
    "#### Let's try to create an embedding of a simple input text\n",
    "You can see its an array of floating point numbers. While it does not make sense to human eye/brain, this array of numbers captures the semantics and knowledge of the text and that can be later used to compare two different text blocks. This method will be used to convert our query to a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43874004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an vector embedding for input text\n",
    "input_text = \"A wine that pairs well with turkey breast?\"\n",
    "\n",
    "embedding = embed_phrase(input_text)\n",
    "\n",
    "#printing text and embedding\n",
    "\n",
    "print(f\"{input_text=}\")\n",
    "\n",
    "#only printing first 10 dimensions of the 1024 dimension vector \n",
    "print(f\"{embedding[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "## 7. Create a index in Amazon MemoryDB \n",
    "Whereas we previously created an index with 2-3 fields, this time we'll define the index with multiple fields: the vectorization of the `description` field, and all others present within the dataset.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the redis client we initiated ealier to create the vector index in MemoryDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define schema\n",
    "schema = (\n",
    "    TextField(\"description\"),\n",
    "    TextField(\"winery\"),\n",
    "    TextField(\"country\"),\n",
    "    TextField(\"designation\"),\n",
    "    TextField(\"variety\"),\n",
    "    TextField(\"points\"),\n",
    "    TextField(\"wine_name\"),\n",
    "    VectorField(\"embedding\", \"FLAT\", {\"TYPE\": \"FLOAT32\", \"DIM\": 1024, \"DISTANCE_METRIC\": \"COSINE\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon MemoryDB. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "redis_conn.ft('wine_index').create_index(fields=schema, definition=IndexDefinition(prefix=['doc:'], index_type=IndexType.HASH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "redis_conn.ft('wine_index').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "## 8. Load the raw data into the Index\n",
    "Next, let's load the wine review data and embedding into the index we've just created. Notice that we will store our embedding in `description_vector` field which will later be used for KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(client, df):\n",
    "    p = client.pipeline(transaction=False)\n",
    "    for i, row in df.iterrows():\n",
    "        key = f\"doc:{row['record_id']}\"\n",
    "        data = {\n",
    "            \"description\": row['description'] or '',\n",
    "            \"winery\": row['winery'] or '',\n",
    "            \"country\": row['country'] or '',\n",
    "            \"designation\": row['designation'] or '',\n",
    "            \"variety\": row['variety'] or '',\n",
    "            \"points\": row['points'] or '',\n",
    "            \"wine_name\": row['title'] or '',            \n",
    "            \"embedding\": np.array(row['embedding'], dtype=np.float32).tobytes()\n",
    "        }\n",
    "        p.hset(key, mapping=data)\n",
    "    p.execute()\n",
    "\n",
    "# loading data\n",
    "load_vectors(redis_conn, df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = redis_conn.ft('wine_index').info()\n",
    "print(\"Records found: %d.\" % result['num_indexed_vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "## 9. Search vector\n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `search_vector` embeds the search phrase, searches the index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vectors(client, query, k=3):\n",
    "    query_vector = np.array(embed_phrase(query), dtype=np.float32).tobytes()\n",
    "\n",
    "    search_query = f\"*=>[KNN {k} @embedding $query_vector AS score]\"\n",
    "    params = {\"query_vector\": query_vector}\n",
    "    result = client.ft('wine_index').search(\n",
    "        Query(search_query)\n",
    "        .sort_by(\"score\")\n",
    "        .return_fields(\"description\", \"winery\", \"country\", \"designation\", \"variety\", \"points\", \"wine_name\")\n",
    "        .dialect(2),\n",
    "        query_params=params\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for doc in result.docs:\n",
    "        results.append({\n",
    "            \"description\": doc.description,\n",
    "            \"winery\": doc.winery,\n",
    "            \"country\": doc.country,\n",
    "            \"designation\": doc.designation,\n",
    "            \"variety\": doc.variety,\n",
    "            \"points\": doc.points,\n",
    "            \"wine_name\": doc.wine_name            \n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "results = search_vectors(redis_conn, query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "## 10. Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with static data, and it does not have our wine review knowledge. While it may be able to answer, it may not be an answer that a business prefers. For example. in our case, we would not want to recommend a wine that we do not stock. So the recommendation has to be one of the wines from our collection i.e. 300 reviews that we loaded. \n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions without the wine review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(system, user_question):\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_question}]\n",
    "    })\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=payload.encode('utf-8'),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Let's check the generated result for a wine recommendation. It may not be one of the wine that we stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "    \n",
    "    #Claude model has 2 parts of the prompt. System prompt guides the model what role to play\n",
    "    system_prompt = f\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    \n",
    "    #User prompt is the engineer prompt that has the context that model should reference to answer questions\n",
    "    user_prompt = (\n",
    "        f\" As a sommelier, you must include the wine variety, the country of origin, \"\n",
    "        f\"and a colorful description relating to the customer question.\"\n",
    "        f\"\\n Customer question: {question}\"\n",
    "        f\"\\n Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"\n",
    "    )\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "print(f\"The recommened wine from LLM without RAG: \\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c5a4c",
   "metadata": {},
   "source": [
    "#### Testing for hallucination. \n",
    "Let's copy the wine name from the last line and past it in the question variable below to see if we have this wine in our stock. Please review the list of wines that are returned. They may be from portugal but not exactly the one we have been recommended by the model.\n",
    "\n",
    "__Note:__ If you do not see the same wine name in the `wine_name` variable below, you should replace it so we can verify that the wine recommended is not in our index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff031f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_name = \"Penfolds Bin 389 Kalimna Shiraz\"\n",
    "example_request = query_llm_without_rag(wine_name)\n",
    "print(\"Matching wine records in our reviews:\")\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "## 11. Retrieval Augmented Generation\n",
    "---\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "#### Create a prompt for the LLM using the search results from MemoryDB (RAG)\n",
    "\n",
    "We will be using the Anthropic Sonnet model with one-shot prompting technique. Within instructions to the model in the prompt, we will provide a sample wine review and how model should use to answer user's question. At the end of the prompt wine reviews retrieved from MemoryDB will be included for model to use. \n",
    "\n",
    "Before querying the model, the below function `generate_rag_based_system_prompt` is used to put together user prompt. The function takes in an input string to search the MemoryDB cluster for a matching wine, then compose the user prompt for LLM. \n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "**SYSTEM PROMPT:**\n",
    "\n",
    "```\n",
    "You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy. \n",
    "```\n",
    "\n",
    "\n",
    "**USER PROMPT**\n",
    "```\n",
    "As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user's question.\n",
    "\n",
    "Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Question from the user as is\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### package the prompt and query the LLM\n",
    "We will create a final function to query the LLM with the prompt. `query_llm_with_rag` is a function that calls LLM in a RAG.\n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- searches the vector index for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = search_vectors(redis_conn, user_question, 10)\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt = \"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    user_prompt = (\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not recommend any wine outside of the wine data provided and you must not suggest further. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Example Wine data: {one_shot_description_example} \\n Example Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents}\\n\"\n",
    "        f\"Customer Question: {user_question}\\n\"\n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc562a1e",
   "metadata": {},
   "source": [
    "#### Let's change it to Italian wine - it should produce a matching result.\n",
    "We will call the same method again to see if there is an italian wine in our catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791272",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Italian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5fe7f",
   "metadata": {},
   "source": [
    "You might notice that we asked for Australian wines that goes well with steak and we do not have any such wine in our collection. Therefore the model politely excuses. You may change the question and see how LLM recommends a wine from our select list that best suites your question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab you built a simple wine recommendation chatbot. In this particular lab you used Amazon Bedrock titan v2 model to create vector embedding for our data. Then we loaded this data in OpenSearch index with `description_vector` field. At search time, we used Amazon Titan v2 model again to convert our query question into vector embedding and used semantic search to retrieve results. These results were then passed on to Anthropic Claude Sonnet 3 model which was able to recommend us a wine from within our catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e244d",
   "metadata": {},
   "source": [
    "## Lab finished - you may now go back to lab instructions section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
