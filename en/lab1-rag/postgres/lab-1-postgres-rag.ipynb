{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# RAG using Aurora PostgreSQL (pgvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "[Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. \n",
    "\n",
    "In this lab we are going to be writing a RAG application code that allows user to ask questions about various wines so they can make a purchasing decision. We will use the semantic search (*vector search*) capability within Aurora PostgreSQL to retrieve the best matching wine reviews and provide that to LLM for answering user's questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "#### a. Download and install python dependencies\n",
    "\n",
    "For this notebook we require the use of a few libraries. We'll use the Python clients for PostgreSQL and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd01d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install deprecated --quiet\n",
    "\n",
    "#PostgreSQL Python Library\n",
    "# !sudo yum install postgresql -y\n",
    "# !pip install psycopg2  --quiet\n",
    "!pip install psycopg2-binary  --quiet\n",
    "#Progress bar for for loop\n",
    "!pip install alive-progress  --quiet\n",
    "\n",
    "# Let's import PyTorch and confirm that the latest version of PyTorch is running. \n",
    "# The version should already be 1.13.1 or higher. If not, it will restart the kernel.\n",
    "\n",
    "import torch\n",
    "pytorch_version = torch.__version__\n",
    "print( f\"Pytorch version: {pytorch_version}\")\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "if pytorch_version.startswith('1.1'):\n",
    "    from IPython.display import display_html\n",
    "    restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 3. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459fff5",
   "metadata": {},
   "source": [
    "#### Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a cloudformation stack in the account. Information of these resources will be used within this lab. We are going to load some of the information variables here.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "apg_cluster = outputs['AuroraDBClusterEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "## 3. Prepare data\n",
    "Below is the code that loads dataset of wine reviews, we'll use this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.json\"\n",
    "df = pd.read_json(url)\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "## 4. Create a connection with Aurora PostgreSQL Cluster.\n",
    "Next, we'll use Python API to set up connection with Aurora PostgreSQL Cluster, our vector database for this workshop.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "To avoid hard coding the user name and password in our code, we have dynamically generated a username and password at the time of deploying the cluster. This user name and password is stored in AWS Secrets Manager service. We will retrieve secret from secrets manager to establish database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "db_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "# For this lab we will use credentials that we have already created in AWS Secrets manager service. Secrets\n",
    "# manager service allows you to store secrets securily and retrieve it through code in a safe manner.\n",
    "\n",
    "pguser = db_credentials['username']\n",
    "pgpass = db_credentials['password']\n",
    "print(pguser,pgpass)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"In the event to troubleshoot APG database, login to the database from Termimal with below info:\")\n",
    "print (\"psql 'postgresql://\"+pguser+\"/\"+pgpass+\"@\"+apg_cluster+\"/vectordb'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e47296",
   "metadata": {},
   "source": [
    "#### Enable and Verify SageMaker Notebook instance to Aurora Cluster connectivity\n",
    "\n",
    "Sagemaker instance and Aurora PostgreSQL (APG) instances are deployed in different VPCs for the workshop. Enable the ingress rules to allow remote traffic and test the connectivity using sample python program below as part of preparation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b3922",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this to add the ingress rules to allow remote access to PostgreSQL\n",
    "!aws ec2 authorize-security-group-ingress --group-name default --protocol tcp --port 5432 --cidr 0.0.0.0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this to verify whether database connectivity works or not\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass)\n",
    "    print (\"Test connection works.\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"I am unable to connect to the database!\\n\")\n",
    "\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe3d",
   "metadata": {},
   "source": [
    "## 5. Using Amazon Bedrock Titan embedding to convert text to vectors\n",
    "Amazon Bedrock offers Amazon Titan embedding v2 model that generates vector embeddings for text. This model will be used as our primary model for embeddings.\n",
    "\n",
    "#### Helper method to invoke Titan embedding model in Amazon Bedrock\n",
    "Creating a helper method in python to invoke Amazon Titan embedding model from Amazon Bedrock to generate embeddings. We will update `df_sample` data frame and add a new column called `embedding` in it. Once this cell is executed, our data frame will be ready to load into our Aurora PostgreSQL database, which supports vector data type and vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{region}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def add_embeddings_to_df(df, text_column):\n",
    "\n",
    "    # Create an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over the text in the specified column\n",
    "    for text in df[text_column]:\n",
    "        embedding = embed_phrase(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_phrase( text ):\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"  # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Extract the embedding from the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "\n",
    "    # Append the embedding to the list\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "df_sample = add_embeddings_to_df(df_sample, 'description')\n",
    "\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875df0b",
   "metadata": {},
   "source": [
    "#### Let's try to create an embedding of a simple input text\n",
    "You can see its an array of floating point numbers. While it does not make sense to human eye/brain, this array of numbers captures the semantics and knowledge of the text and that can be later used to compare two different text blocks. This method will be used to convert our query to a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43874004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an vector embedding for input text\n",
    "input_text = \"A wine that pairs well with turkey breast?\"\n",
    "\n",
    "embedding = embed_phrase(input_text)\n",
    "\n",
    "#printing text and embedding\n",
    "\n",
    "print(f\"{input_text=}\")\n",
    "\n",
    "#only printing first 10 dimensions of the 1024 dimension vector \n",
    "print(f\"{embedding[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "## 7. Create a table in Amazon Aurora for PostgreSQL  \n",
    "\n",
    "We will create the a table to store the wine data. It contains the vectorization of the `description` field, and all others present within the dataset.\n",
    "\n",
    "In this step, we first create the pgvector extension and the table, as well as index to speed up the similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "with psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass) as conn:\n",
    "    curs = conn.cursor()\n",
    "    curs.execute (\"create extension if not exists vector\")\n",
    "    curs.execute (\"drop table if exists tbl_wine\")\n",
    "    curs.execute (\"create table tbl_wine (id bigserial primary key,\"+\n",
    "                 \"description text, designation text, variety text,\"+\n",
    "                 \"country text,winery text,points int,wine_name text, description_vector vector(1024))\")\n",
    "    curs.execute (\"create index tbl_wine_vector on tbl_wine \"+\n",
    "                 \"using hnsw(description_vector vector_cosine_ops) \"+\n",
    "                 \"with (m = 24, ef_construction = 100)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Now we have created a table `tbl_wine` and index `tbl_wine_vector` to speed up the vector search. Execute follow query to confirm our table created properly with empty data.\n",
    "Click [pgvector](https://github.com/pgvector/pgvector) if you are interested to learn more about pgvector capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "with psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass) as conn:\n",
    "    curs = conn.cursor()\n",
    "    curs.execute (\"select * from tbl_wine\")\n",
    "    print (\"Query the table succesfully with expected 0 row (actual rowcount: \"+ str(curs.rowcount) +\").\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "## 8. Load the raw data into the Index\n",
    "Next, let's load the wine review data and embedding into the index we've just created. Notice that we will store our embedding in `description_vector` field which will later be used for KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863660",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = '''insert into tbl_wine(description_vector,\n",
    "        description,points,variety,country,designation,winery,wine_name) \n",
    "        values(%s,%s,%s,%s,%s,%s,%s,%s)'''\n",
    "with psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass) as conn:\n",
    "    curs = conn.cursor()\n",
    "    curs.execute (\"truncate table tbl_wine\") # empty the table first before data loading\n",
    "    for index, record in tqdm(df_sample.iterrows()):\n",
    "        row=(record['embedding'],record[\"description\"],record[\"points\"],record[\"variety\"],\n",
    "           record[\"country\"],record[\"designation\"],record[\"winery\"],record[\"title\"])\n",
    "        curs.execute (stmt,row)\n",
    "print(\"Records loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of rows in the table `tbl_wine`. We should have 300 rows there, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass) as conn:\n",
    "    curs = conn.cursor()\n",
    "    curs.execute (\"select * from tbl_wine\")\n",
    "    print (\"Query the table succesfully with expected 300 row (actual rowcount: \"+ str(curs.rowcount) +\").\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "## 9. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `retrieve_apg_with_semantic_search` embeds the search phrase, searches the table in Aurora PostgreSQL (APG) for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_apg_with_semantic_search(phrase, n=3):\n",
    "    search_vector = embed_phrase(phrase)\n",
    "\n",
    "    stmt = '''select description,winery,points,designation,country,variety,wine_name,\n",
    "              1 - (description_vector <=> cast(%s as vector)) AS cosine_similarity\n",
    "              from tbl_wine order by cosine_similarity desc limit %s'''\n",
    "    results = []\n",
    "    \n",
    "    with psycopg2.connect(host=apg_cluster,port=5432,dbname='vectordb',user=pguser, password=pgpass) as conn:\n",
    "        curs = conn.cursor()\n",
    "        curs.execute (stmt,(search_vector,n))\n",
    "    \n",
    "        for record in curs:\n",
    "            result = {\n",
    "                \"description\":record[0],\n",
    "                \"winery\":record[1],\n",
    "                \"points\":record[2],\n",
    "                \"designation\":record[3],\n",
    "                \"country\":record[4],\n",
    "                \"variety\":record[5],\n",
    "                \"wine_name\":record[6],\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "example_request = retrieve_apg_with_semantic_search(question_on_wine)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "## 10. Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with static data, and it does not have our wine review knowledge. While it may be able to answer, it may not be an answer that a business prefers. For example. in our case, we would not want to recommend a wine that we do not stock. So the recommendation has to be one of the wines from our collection i.e. 300 reviews that we loaded. \n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions without the wine review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "    # Define the prompt for the model\n",
    "    prompt = \"Write a sonnet about the beauty of nature.\"\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Let's check the generated result for a wine recommendation. It may not be one of the wine that we stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "    \n",
    "    #Claude model has 2 parts of the prompt. System prompt guides the model what role to play\n",
    "    system_prompt = f\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    \n",
    "    #User prompt is the engineer prompt that has the context that model should reference to answer questions\n",
    "    user_prompt = (\n",
    "        f\" As a sommelier, you must include the wine variety, the country of origin, \"\n",
    "        f\"and a colorful description relating to the customer question.\"\n",
    "        f\"\\n Customer question: {question}\"\n",
    "        f\"\\n Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"\n",
    "    )\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "print(f\"The recommened wine from LLM without RAG: \\n\\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c5a4c",
   "metadata": {},
   "source": [
    "#### Testing for hallucination. \n",
    "Let's copy the wine name from the last line and past it in the question variable below to see if we have this wine in our stock. Please review the list of wines that are returned. They may be from portugal but not exactly the one we have been recommended by the model.\n",
    "\n",
    "__Note:__ If you do not see the same wine name in the `wine_name` variable below, you should replace it so we can verify that the wine recommended is not in our Aurora PostgreSQL `tbl_wine` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff031f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_name = \"Penfolds Bin 28 Kalimna Shiraz\"\n",
    "example_request = retrieve_apg_with_semantic_search(wine_name)\n",
    "print(\"Matching wine records in our reviews:\")\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "## 11. Retrieval Augmented Generation\n",
    "---\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "#### Create a prompt for the LLM using the search results from Aurora PostgreSQL (RAG)\n",
    "\n",
    "We will be using the Anthropic Sonnet model with one-shot prompting technique. Within instructions to the model in the prompt, we will provide a sample wine review and how model should use to answer user's question. At the end of the prompt wine reviews retrieved from Aurora Postgre will be included for model to use. \n",
    "\n",
    "Before querying the model, the below function `generate_rag_based_system_prompt` is used to put together user prompt. The function takes in an input string to search the `description_vector` column in `tbl_wine` table for a matching wine, then compose the user prompt for LLM. \n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "**SYSTEM PROMPT:**\n",
    "\n",
    "```\n",
    "You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy. \n",
    "```\n",
    "\n",
    "\n",
    "**USER PROMPT**\n",
    "```\n",
    "As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user's question.\n",
    "\n",
    "Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Question from the user as is\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### package the prompt and query the LLM\n",
    "We will create a final function to query the LLM with the prompt. `query_llm_with_rag` is a function that calls LLM in a RAG.\n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- searches the Aurora PostgreSQL vector data with semantic search for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_apg_with_semantic_search(user_question)\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt= \"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy\"\n",
    "    user_prompt = (\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not recommend any wine that is not in the wine data provided, do not suggest anything further. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Wine data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents} \\n\"\n",
    "        f\"Customer Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_apg_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc562a1e",
   "metadata": {},
   "source": [
    "#### Let's change it to Italian wine - it should produce a matching result.\n",
    "We will call the same method again to see if there is an italian wine in our catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791272",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Italian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_apg_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5fe7f",
   "metadata": {},
   "source": [
    "You might notice that we asked for Australian wines that goes well with steak and we do not have any such wine in our collection. Therefore the model politely excuses. You may change the question and see how LLM recommends a wine from our select list that best suites your question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab you built a simple wine recommendation chatbot. In this particular lab you used Amazon Bedrock titan v2 model to create vector embedding for our data. Then we loaded this data in OpenSearch index with `description_vector` field. At search time, we used Amazon Titan v2 model again to convert our query question into vector embedding and used semantic search to retrieve results. These results were then passed on to Anthropic Claude Sonnet 3 model which was able to recommend us a wine from within our catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046fe95",
   "metadata": {},
   "source": [
    "## Lab finished - you may now go back to lab instructions section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
