{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb05169",
   "metadata": {},
   "source": [
    "# Advanced RAG with OpenSearch neural plugin \n",
    "This lab will walk you an advanced RAG architecture where we will process a PDF document. We will first extract text from PDF, then we will chunk it using recursive character chunking technique. We will use OpenSearch neural plugin to help us convert these chunks into vector at the time of ingestion. Once data is ingested, we will use Neural plugin to semantically search the data. We will use the returned results to engineer a prompt to generate answers.\n",
    "\n",
    "The PDF that we will is an annual report for 2023 published by Amazon. Within this document there is financial data, and there is performance, risks facing the company and guidance for future. We will be a financial analyst assistant bot that may answer questions from this document.\n",
    "\n",
    "We will start by loading appropriate libraries.\n",
    "\n",
    "## 1. Install pre-requisites and initialize variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fb6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community pypdf langchain_experimental --quiet\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install --upgrade --quiet  boto3\n",
    "!pip install pdfminer.six --quiet\n",
    "!pip install opensearch-py --quiet\n",
    "!pip install \"unstructured[all-docs]\" --quiet\n",
    "!pip install pdf2image --quiet\n",
    "!pip install -qU langchain-aws --quiet\n",
    "!pip install alive_progress --quiet\n",
    "!pip install opensearch-py-ml --quiet\n",
    "!pip install requests_aws4auth --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1915eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests \n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd7142",
   "metadata": {},
   "source": [
    "### Initializing variables from CloudFormation stack output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87760ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86734",
   "metadata": {},
   "source": [
    "## 2. Extracting text from PDF and chunking\n",
    "The most simplest way to chunk document would be by length, but keeping paragraphs or lines together so it does not lose the meaning. We will use langchain library's recursive character text splitter which offers ways to split data by length, yet keeps the lines, paragraph together as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a82f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this method would split the text into chunks by paragraph, line boundary and keeping chunk \n",
    "# size as close to 1000 characters, it will also overlap the text between chunks if it were to \n",
    "# split line or paragraph in the middle.\n",
    "\n",
    "def recursive_character_chunking(text): \n",
    "    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=200, #number of characters that can overlap with previous chunk\n",
    "        length_function=len,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.create_documents(text)#From the loaded PDF\n",
    "    \n",
    "    return docs #return the index to be cached by the client app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b894d",
   "metadata": {},
   "source": [
    "### Load and parse PDF file.\n",
    "Now we will use Langchain library's PyPDFLoader to load our PDF and extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD A PDF DOCUMENT\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Amazon-com-Inc-2023-Annual-Report.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#print(documents)a\n",
    "\n",
    "texts = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    texts += document.page_content.replace(r'\\\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8187e",
   "metadata": {},
   "source": [
    "Now we will chunk the loaded PDF text using recursive character chunking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LETS RECURISIVE CHUNK IT\n",
    "docs = recursive_character_chunking([texts])\n",
    "\n",
    "# the method prints chunks\n",
    "def print_chunks(data):\n",
    "    #Let's print the chunks -- notice the overlap between chunk 3 and 4\n",
    "    i = 1\n",
    "    for doc in data:\n",
    "        print(f\"---------START OF CHUNK {i}------\")\n",
    "        print(f\"{doc.page_content}\")\n",
    "        print(f\"---------END OF CHUNK {i}------\\n\\n\")\n",
    "        i+=1\n",
    "\n",
    "#Let's print first 5 chunks.\n",
    "print_chunks(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f4a6d",
   "metadata": {},
   "source": [
    "## 3. Create a connection with OpenSearch domain.\n",
    "Next, we'll use Python API to set up connection with OpenSearch domain.\n",
    "\n",
    "\n",
    "##### NOTE: \n",
    "_At any point in this lab, if you get a failure message - **The security token included in the request is expired.**_ You can resolve it by running this cell again. The cell refreshes the security credentials that is required for the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59694500",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "host = f'https://{aos_host}/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing some variables that we will use later.\n",
    "\n",
    "connector_id = \"\"\n",
    "model_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449368b",
   "metadata": {},
   "source": [
    "## 4. Create and deploy model connector to Amazon Bedrock Titan Text Embedding v2 \n",
    "\n",
    "We are going to use Amazon Sagemaker Notebook IAM role to configure the connector in OpenSearch. This IAM Role has permission to pass BedrockInference IAM role to OpenSearch. OpenSearch will then be able to use BedrockInference IAM role to make calls to Bedrock models.\n",
    "\n",
    "Following cell will create a connector using SageMaker Notebook IAM role. Following cell will create a OpenSearch remote model connector with Amazon Bedrock Titan Text embedding v2 model. Following cell defines the connector configuration.\n",
    "\n",
    "\n",
    "#### Important pre-requisite\n",
    "You should have followed the steps in the Lab instruction section to map Sagemaker notebook role to OpenSearch `ml_full_access` role. If not, please visit the lab instructions and complete the **Setting up permission for Notebook IAM Role** section. If you have not done this, you will get an error in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "\n",
    "if not connector_id:\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan text embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "           ## USING AMAZON BEDROCK TITAN EMBED TEXT MODEL\n",
    "        \"model\": \"amazon.titan-embed-text-v2:0\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "         \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "         \"post_process_function\": \"connector.post_process.bedrock.embedding\"}\n",
    "       ]\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    if r.status_code == 403:\n",
    "        print(\"Permission Error: Please make sure you have mapped the NB role to ml_full_access role in OpenSearch dashboard. Follow permission section in lab instructions.\")\n",
    "        print(r.text)\n",
    "    else:\n",
    "        connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "        print(r.text)\n",
    "else:\n",
    "    print(f\"Connector already exists - {connector_id}\")\n",
    "    \n",
    "connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89985115",
   "metadata": {},
   "source": [
    "Once the model connector is defined. We need to register the model and deploy. Following two cells will register and then deploy the model connection respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "if not model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Bedrock Titan embeddings model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"Bedrock Titan text embeddings model\",\n",
    "    \"connector_id\": connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - model already exists\")\n",
    "print(\"Model registered under model_id: \"+model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7676c",
   "metadata": {},
   "source": [
    "finally we will deploy the model which will enable the remote inference. This model is what we will use to conver text to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14098c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "path = '_plugins/_ml/models/'+model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a896856",
   "metadata": {},
   "source": [
    "#### Create a test embedding to see model that is deployed is working fine.\n",
    "\n",
    "Lets use `ml_client.generate_embedding` method which is a method in OpenSearch ML Commons plugin python client to call an embedding model to generate embeddings.\n",
    "\n",
    "**Import pre-requisite**: Please make sure you have followed _**Provision Amazon Bedrock model access**_ section in the lab instruction to setup Amazon Bedrock model access. If you have not done so, you may get an Authorization Exception when you run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model working\n",
    "\n",
    "input_sentences = [\"What an easy way to create embeddings\"]\n",
    "embedding_output = ml_client.generate_embedding(f\"{model_id}\", input_sentences)\n",
    "embed = embedding_output['inference_results'][0]['output'][0]['data']\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67095fe2",
   "metadata": {},
   "source": [
    "## 5. Create ingest pipeline\n",
    "Let's create an ingestion pipeline that will call Amazon Bedrock Titan Text embedding model and convert the `doc_chunk_text` field of the text chunk record to vector embedding. Ingest pipeline is a feature in OpenSearch that allows you to define certain actions to be performed at the time of data ingestion. You could do simple processing such as adding a static field, modify an existing field, or call a remote model to get inference and store inference output together with the indexed record/document. In our case inference output is vector embedding.\n",
    "\n",
    "Following ingestion pipeline is going to call our remote model and convert chunk text `doc_chunk_text` field to vector and store it in the field called `doc_chunk_embedding`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_ingest/pipeline/amazon-report-ingest-pipeline\"\n",
    "url = f\"{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"An Index of Amazon annual report\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": f\"{model_id}\",\n",
    "        \"field_map\": {\n",
    "          \"doc_chunk_text\": \"doc_chunk_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "aos_client.ingest.put_pipeline(id=\"amazon-report-ingest-pipeline\", body=payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ad347",
   "metadata": {},
   "source": [
    "## 6. Create a index in Amazon Opensearch Service \n",
    "Now we will define our index for our PDF document knowledgebase. We are going to define 2 fields, first text chunk and second has the vector embeddings. For vector embedding, we will use `FAISS` engine and `HNSW` as our algorithm/method. We will use some reasonable defaults for other parameters. Notice that we provide the above created pipeline as the default pipeline for the data ingested into index. This will make the documents go through pipeline before being ingested in the index.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEFINE INDEX JSON\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"default_pipeline\": \"amazon-report-ingest-pipeline\", \n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "             \"doc_chunk_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "           \"doc_chunk_embedding\": {\n",
    "               \"type\": \"knn_vector\",\n",
    "               \"dimension\": 1024,\n",
    "               \"method\": {\n",
    "                   \"name\": \"hnsw\",\n",
    "                   \"space_type\": \"l2\",\n",
    "                   \"engine\": \"faiss\",\n",
    "                   \"parameters\": {\n",
    "                       \"ef_construction\": 256,\n",
    "                       \"m\": 48\n",
    "                   }\n",
    "               }\n",
    "           }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e30846",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch Service. Running this cell will recreate the index if you have already executed this notebook before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"amazon_report_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585828d3",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e700a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124671d5",
   "metadata": {},
   "source": [
    "## 7. Load the raw data into the Index\n",
    "Next, let's load the chunked text data and its embeddings into the index that we've just created. Notice that we will store our embedding in `doc_chunk_embedding` field which will later be used for KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "batch = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": index_name } })\n",
    "body_ = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(docs), force_tty = True) as bar:\n",
    "    for doc in docs:\n",
    "\n",
    "        payload={\n",
    "           \"doc_chunk_text\": doc.page_content\n",
    "        }\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        if(cnt == 100):\n",
    "            \n",
    "            response = aos_client.bulk(\n",
    "                                index = index_name,\n",
    "                                 body = body_)\n",
    "            \n",
    "\n",
    "            cnt = 0\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8d61a",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have close to 400 documents in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c227db",
   "metadata": {},
   "source": [
    "## 8. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a chunk which matches the user's question semantically. `retrieve_opensearch_with_semantic_search` uses neural query which takes the query text and the model id that it will use to convert the query text into embedding before running a approximate neighbour search i.e. semantic search. We pass K as the parameter to tell how many results we want openseach to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d499e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"doc_chunk_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{model_id}\",\n",
    "            \"k\": n\n",
    "          }\n",
    "        }\n",
    "      }    \n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bb79e",
   "metadata": {},
   "source": [
    "### Use the semantic search to get similar records with the sample question.\n",
    "\n",
    "We will ask a question that we think can be answered from our chunked data. OpenSearch will return top results that match the question. You will notice the chunks returned do talk about the topics in the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86275ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs = \"What was the operating income difference between 2022 and 2023?\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_docs)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f3b0a",
   "metadata": {},
   "source": [
    "## 9. Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model\n",
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with general purpose data, likely before the PDF was produced. It may not have the knowledge hidden in the PDF file. While it may be able to answer, it may not be an answer that your business prefers. So the answer has to reference the data that we passed in the prompt. \n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions from the PDF chunk data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d844b",
   "metadata": {},
   "source": [
    "## 10. Retrieval Augmented Generation\n",
    "\n",
    "#### Create a prompt for the LLM using the search results from OpenSearch (RAG)\n",
    "\n",
    "We will be using the Anthropic Sonnet model with one-shot prompting technique. Within instructions to the model in the prompt. At the end of the prompt we will provide the chunks retrieved from Opensearch for model to refer to, to answer user's questions. \n",
    "\n",
    "Before querying the model, the below function `generate_rag_based_system_prompt` is used to put together user prompt. The function takes in an input string to search the OpenSearch cluster for a matching chunk, then composes the user prompt for LLM. \n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "**SYSTEM PROMPT:**\n",
    "\n",
    "```\n",
    "You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions. \n",
    "```\n",
    "\n",
    "\n",
    "**USER PROMPT**\n",
    "```\n",
    "As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\n",
    "\n",
    "Following is the DOCS_DATA after which you will be given the user's question to answer.\n",
    "\n",
    "DOCS_DATA: {retrieved_documents}\n",
    "\n",
    "User's Question: <USER QUESTION>\n",
    "```\n",
    "\n",
    "We define a method `query_llm_with_rag` that combines everything we've done in this module. It does all of the following:\n",
    "- searches the OpenSearch index with semantic search for the relevant chunk.\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa520f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question, n=3):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(user_question,n)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152590e",
   "metadata": {},
   "source": [
    "### Answer questions using RAG\n",
    "Lets ask a question that can be answered from the PDF text data. We encourage you to change the question to other financial or general company performance related questions which could possibly be answered from the annual report. You will notice that this simple architecture can provide you really good way to analyze a complex document like the annual financial report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What was the revenue for amazon advertisement in 2023?\"\n",
    "recommendation = query_llm_with_rag(question_on_docs)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350cf368",
   "metadata": {},
   "source": [
    "# 11. Hybrid search - Advanced RAG\n",
    "following section will explore how Hybrid search may help in certain situations. \n",
    "\n",
    "In Amazon report **knowledge bases** is mentioned as the technology based on RAG or retrieval augmented generation to answer real time queries.\n",
    "\n",
    "Let's use following question as an example to search for AWS offering for RAG. The semantic meaning of RAG may not be known to the embedding model, which is why it would use other terms in the user's query to match information in the chunks. This may result in LLM not able to answer our question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\"\n",
    "recommendation = query_llm_with_rag(question_on_docs, n=3)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs, n=3), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22610728",
   "metadata": {},
   "source": [
    "### How do we solve this?\n",
    "As you can see the top 3 chunks do not contain the information that could answer user's question. In such cases vector search may find a number of similar looking chunks, however, just by looking at similarity distance, it may not return the chunk that has the answer, to the top. We could augment semantic search with traditional keyword search and combine the results. This is called **Hybrid search**. The results from both keywords and semantic search are merged with various normalization and combination techniques to produce final results. Let us now define a search pipeline that uses **_phase_results_processors_** results processor that takes 2 different queries and combine their results. In our case we will give 40% weightage to keyword matches and 60% to semantic matches, and results are combined using **harmonic mean** technique. \n",
    "\n",
    "Our queries will be a full text search and a semantic search as before. We rewrite our retrieval method and add keyword query, we call this method _retrieve_opensearch_with_hybrid_search( )_, notice how this method has 2 query clauses under \"**hybrid**\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_search/pipeline/hybrid-search-pipeline\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"Hybrid search over amazon financial report\",\n",
    "  \"phase_results_processors\":[\n",
    "      {\n",
    "          \"normalization-processor\":{\n",
    "              \"normalization\": {\n",
    "                  \"technique\": \"l2\"\n",
    "              },\n",
    "              \"combination\": {\n",
    "                  \"technique\": \"harmonic_mean\",\n",
    "                  \"parameters\": {\n",
    "                      \"weights\": [\n",
    "                          0.4, #first query i.e. keywords have 20% weightage\n",
    "                          0.6  #first query i.e. semantic search have 80% weightage\n",
    "                      ]\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_opensearch_with_hybrid_search(phrase, model_id=model_id, bedrock_client=bedrock_client, n=3 ):\n",
    "    osquery={\n",
    "            \"_source\": {\n",
    "                \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "            },\n",
    "          \"size\": n,\n",
    "          \"query\": {\n",
    "            \"hybrid\": {\n",
    "              \"queries\": [\n",
    "                {\n",
    "                  #First query clause that performs keyword search\n",
    "                  \"match\": {\n",
    "                    \"doc_chunk_text\": {\n",
    "                      \"query\": f\"{phrase}\"\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  #Second query clause that performs semantic search\n",
    "                  \"neural\": {\n",
    "                    \"doc_chunk_embedding\": {\n",
    "                      \"query_text\": f\"{phrase}\",\n",
    "                      \"model_id\": f\"{model_id}\",\n",
    "                      \"k\": n\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }    \n",
    "        }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           search_pipeline=\"hybrid-search-pipeline\",\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"id\":entry['_id'],\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text'],\n",
    "            \"_score\":entry['_score']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def hybrid_query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_opensearch_with_hybrid_search(user_question)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41384fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\"\n",
    "recommendation = hybrid_query_llm_with_rag(question_on_docs)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_hybrid_search(question_on_docs, model_id=model_id, bedrock_client=bedrock_client), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3dcb6b",
   "metadata": {},
   "source": [
    "### Hybrid search is popular but...\n",
    "We are seeing customers use hybrid search often to find relevant results when only semantic search may not be the only solution. This highly applicable if your use case has domain specific terminologies (e.g. RAG) that embedding model may not have seen enough in training data or its very specific to your company taxonomy and might mean very differently to that of its traditional meaning.\n",
    "\n",
    "It could also be that the top 3 results that semantic search found do not have the answer. Let's try semantic search again, this time with **n=10** i.e. top 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\"\n",
    "\n",
    "#Setting n=10 to retrieve 10 results instead of 3\n",
    "n = 10\n",
    "recommendation = query_llm_with_rag(question_on_docs, n)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs, n=n), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e63d1",
   "metadata": {},
   "source": [
    "As you can see model could answer the question a little better.\n",
    "\n",
    "## 12. Re-ranking using a cross encoder model \n",
    "\n",
    "Above problem can be solved if we can find a way to rerank results in such as a way that text chunk that talks about RAG becomes within the top 3 results. This could potentially be achieved with cross encoder model which help us rerank opensearch results by looking at the query and result sets.\n",
    "\n",
    "Cross encoder models are supported in OpenSearch and two of such models are available in OpenSearch service.\n",
    "\n",
    "Cross encoder model look at the query and search results and rerank them in such a way that results that could possibly answer the query would be ranked higher. \n",
    "\n",
    "### Lets set some cluster setting that enable local model hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56514125",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = b\"\"\"\n",
    "{\"transient\":{\"plugins.ml_commons.only_run_on_ml_node\": false}}\n",
    "\"\"\"\n",
    "aos_client.cluster.put_settings(body=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea76c2",
   "metadata": {},
   "source": [
    "## Deploy cross encoder model\n",
    "We will use ML commons feature in opensearch to load a cross encoder model. This model is hosted within opensearch in a data node. Earlier we used ML commons to deploy a remote inference model that call Amazon Bedrock Titan. This time we will register and deploy a cross encoder model within a data node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47aec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_id=\"\"\n",
    "\n",
    "path =  \"/_plugins/_ml/model_groups/_register\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"local_model_group\",\n",
    "  \"description\": \"A model group for local models\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "model_group_id = json.loads(r.text)[\"model_group_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85b185",
   "metadata": {},
   "source": [
    "Now we will register a model .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1825efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_plugins/_ml/models/_register\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2\",\n",
    "  \"version\": \"1.0.2\",\n",
    "  \"model_group_id\": f\"{model_group_id}\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "rr_model_task_id = json.loads(r.text)[\"task_id\"]\n",
    "\n",
    "rr_model_task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21517e7",
   "metadata": {},
   "source": [
    "Following cell will check if the model registration is completed. Please run it until you see model registered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model_id = \"\"\n",
    "\n",
    "path =  f\"/_plugins/_ml/tasks/{rr_model_task_id}\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.get(url, auth=awsauth, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "task_state = json.loads(r.text)[\"state\"]\n",
    "if task_state == \"COMPLETED\":\n",
    "    rr_model_id = json.loads(r.text)[\"model_id\"]\n",
    "    print(\"TASK COMPLETED SUCCESSFULLY, PLEASE MOVE TO NEXT CELL\")\n",
    "else:\n",
    "    print(\"TASK NOT COMPLETED, PLEASE RE-RUN THIS CELL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33ee8d",
   "metadata": {},
   "source": [
    "Following code will deploy the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  f\"/_plugins/_ml/models/{rr_model_id}/_deploy\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "rr_model_task_id = json.loads(r.text)[\"task_id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb185ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  f\"/_plugins/_ml/tasks/{rr_model_task_id}\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.get(url, auth=awsauth, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "task_state = json.loads(r.text)[\"state\"]\n",
    "if task_state == \"COMPLETED\":\n",
    "    print(\"MODEL DEPLOYMENT SUCCESSFUL, PLEASE MOVE TO THE NEXT CELL\")\n",
    "    rr_model_id = json.loads(r.text)[\"model_id\"]\n",
    "elif task_state == \"FAILED\":\n",
    "    print(\"MODEL DEPLOYMENT FAILED, PLEASE INVESTIGATE THE ERROR\")\n",
    "else:\n",
    "    print(\"MODEL DEPLOYMENT IN PROGRESS, PLEASE RE-RUN THE CELL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab23a5",
   "metadata": {},
   "source": [
    "### Create a search pipeline that uses re-ranker model \n",
    "Now we will deploy a search pipeline that will pass the search results to reranker model that we just deployed. The model requires names of the field that cross encoder will use to assess result ranking. In this case we provide `doc_chunk_text` as the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_search/pipeline/rerank-search-pipeline\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"Pipeline for reranking with cross-encoder model\",\n",
    "    \"response_processors\": [\n",
    "        {\n",
    "            \"rerank\": {\n",
    "                \"ml_opensearch\": {\n",
    "                    \"model_id\": f\"{rr_model_id}\"\n",
    "                },\n",
    "                \"context\": {\n",
    "                    \"document_fields\": [\"doc_chunk_text\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(payload)\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62127e",
   "metadata": {},
   "source": [
    "Lets rewrite our semantic search method, we will use rerank clause and provide our query as the query context. With this query and the `doc_chunk_text` we provided as part of search pipeline, cross encoder has everything it needs perform the reranking. We will define following two methods\n",
    "\n",
    "`retrieve_opensearch_with_semantic_rerank_search` - Method runs a semantic search uses rerank clause to rerank results. When it runs search, it uses `rerank-search-pipeline` as the search pipeline.\n",
    "\n",
    "`rerank_query_llm_with_rag` - This method calls rerank search method above to retrieve top chunks and then passes the top chunks to Claude Sonnet 3 to answer user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ef7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_rerank_search(phrase, n, k, model_id=model_id, bedrock_client=bedrock_client):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": k,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"doc_chunk_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{model_id}\",\n",
    "            \"k\": k\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "     \"ext\":{\n",
    "         \"rerank\": {\n",
    "          \"query_context\": {\n",
    "             \"query_text\": f\"{phrase}\"\n",
    "        }\n",
    "     }\n",
    "        \n",
    "    }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           search_pipeline=\"rerank-search-pipeline\",\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "\n",
    "    results = []\n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"id\":entry['_id'],\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text'],\n",
    "            \"_score\":entry['_score']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results[:n]\n",
    "\n",
    "\n",
    "def rerank_query_llm_with_rag(user_question, n, k):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_rerank_search(user_question, n = n, k = k, model_id=model_id, bedrock_client=bedrock_client)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbac57",
   "metadata": {},
   "source": [
    "## Run cross encoder search\n",
    "Following cell runs a search for the same question that could not have been answered. We will fetch K = 10 items, which will be reranked by cross encoder model. From the top K we will pass top N records to the LLM for answering user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56779f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\"\n",
    "\n",
    "#keeping our results to top 3 items.\n",
    "n = 3\n",
    "recommendation = rerank_query_llm_with_rag(question_on_docs, n=3, k=10)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_rerank_search(phrase=question_on_docs, n=3, k=10), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4468ef",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "You started by loading and parsing Amazon annual financial report in PDF file format, chunking the loaded text and using Neural plugin converted chunks into embedding without having to handle the embeddings yourself. Then you used semantic search to answer user's finance related question over this data. We saw how hybrid search can improve our retrieval by combining powerful keyword with novel semantic search capability. We also showed how reranking can be an important enhancement in your RAG pipeline.\n",
    "\n",
    "Congratulations on finsihing this lab. You may now go to the lab instructions section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
