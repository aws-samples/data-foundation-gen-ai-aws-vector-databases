{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32c485b",
   "metadata": {},
   "source": [
    "# 검색 증강 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466050e8",
   "metadata": {},
   "source": [
    "[검색 증강 생성(Retrieval Augmented Generation)](https://arxiv.org/abs/2005.11401)은 검색 기반 모델과 생성 모델을 결합하여 자연어 생성을 향상시키는 과정입니다. 이 과정에서는 관련 정보를 검색하고 이를 생성 프로세스에 통합합니다.\n",
    "\n",
    "이 실습에서는 사용자가 다양한 와인에 대해 질문할 수 있는 RAG 애플리케이션 코드를 작성할 것입니다. 이를 통해 사용자는 구매 결정을 내리는 데 도움을 받을 수 있습니다. OpenSearch의 의미론적 검색(벡터 검색) 기능을 사용하여 가장 잘 일치하는 와인 리뷰를 검색하고, 이를 LLM(Large Language Model)에 제공하여 사용자의 질문에 답변할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a93b6",
   "metadata": {},
   "source": [
    "## 1. 실습 전 준비사항\n",
    "\n",
    "#### a. Python 의존성 다운로드 및 설치\n",
    "\n",
    "이 노트북을 위해 몇 가지 라이브러리가 필요합니다. OpenSearch와 SageMaker를 위한 Python 클라이언트를 사용할 것이며, 텍스트 임베딩을 생성하기 위해 OpenSearch ML 클라이언트 라이브러리를 사용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd01d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.101 requires botocore==1.34.101, but you have botocore 1.34.143 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mPytorch version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install setuptools==70.1.1 --quiet\n",
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install deprecated --quiet\n",
    "\n",
    "\n",
    "#OpenSearch Python SDK\n",
    "!pip install opensearch_py  --quiet\n",
    "#Progress bar for for loop\n",
    "!pip install alive-progress  --quiet\n",
    "\n",
    "# The version should already be 1.13.1 or higher. If not, we will restart the kernel.\n",
    "# 버전은 이미 1.13.1 이상이어야 합니다. 그렇지 않은 경우 커널을 재시작합니다.\n",
    "\n",
    "import torch\n",
    "pytorch_version = torch.__version__\n",
    "print( f\"Pytorch version: {pytorch_version}\")\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "if pytorch_version.startswith('1.1'):\n",
    "    from IPython.display import display_html\n",
    "    restartkernel()\n",
    "    \n",
    "##You may safely ignore pip dependencies errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73c122",
   "metadata": {},
   "source": [
    "#### b. 라이브러리 가져오기 & 리소스 정보 초기화\n",
    "\n",
    "아래 줄은 이 노트북에서 사용되는 모든 관련 라이브러리와 모듈을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365e8c0",
   "metadata": {},
   "source": [
    "#### c. CloudFormation 스택 출력 변수 가져오기\n",
    "\n",
    "우리는 계정에 CloudFormation 스택을 생성하여 몇 가지 리소스를 미리 구성해 두었습니다. 이 리소스들의 이름과 ARN은 이 실습 내에서 사용될 것입니다. 여기서 우리는 이러한 정보 변수들 중 일부를 로드할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Boto3 세션 만들기\n",
    "session = boto3.Session()\n",
    "\n",
    "# Account ID 가져오기\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# 현재 region 가져오기\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Cloudformation 스택에서 출력 변수를 가져오는 메서드입니다. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## 나머지 데모에 사용할 변수를 설정합니다.\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# 필요한 경우 쉽게 복사할 수 있도록 모든 변수를 출력합니다.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d65c28",
   "metadata": {},
   "source": [
    "## 3. 데이터 준비\n",
    "아래는 와인 리뷰 데이터셋을 로드하는 코드입니다. 이 데이터셋을 사용하여 사용자가 제공한 설명과 유사한 와인을 추천할 것입니다.\n",
    "\n",
    "#### OpenSearch에 빠르게 로드하기 위해 레코드의 일부 샘플링\n",
    "데이터가 129,000개의 레코드로 구성되어 있기 때문에, 이들을 벡터로 변환하고 벡터 저장소에 로드하는 데 시간이 걸릴 수 있습니다. 따라서 우리는 데이터의 일부(300개 레코드)만을 사용할 것입니다. 레코드의 인덱스에 해당하는 record_id라는 변수를 추가할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fc342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.json\"\n",
    "df = pd.read_json(url)\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d47f",
   "metadata": {},
   "source": [
    "## 4. Amazon OpenSearch Service 도메인과 연결 생성\n",
    "다음으로, Python API를 사용하여 OpenSearch 도메인과의 연결을 설정할 것입니다.\n",
    "\n",
    "#### Secrets Manager에서 자격 증명 검색\n",
    "코드에 사용자 이름과 비밀번호를 하드코딩하는 것을 피하기 위해, 우리는 클러스터를 배포할 때 사용자 이름과 비밀번호를 동적으로 생성했습니다. 이 사용자 이름과 비밀번호는 AWS Secrets Manager 서비스에 저장되어 있습니다. OpenSearch 연결을 설정하기 위해 Secrets Manager에서 비밀을 검색할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "# 이 실습에서는 AWS Secrets 관리자 서비스에서 이미 생성한 자격 증명을 사용하겠습니다. Secrets\n",
    "# 관리자 서비스를 사용하면 비밀을 안전하게 저장하고 코드를 통해 안전하게 검색할 수 있습니다.\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03475dd",
   "metadata": {},
   "source": [
    "## 5. Amazon Bedrock Titan Text 임베딩을 사용하여 텍스트를 벡터로 변환\n",
    "Amazon Bedrock 서비스는 텍스트에 대한 벡터 임베딩을 생성하는 Amazon Titan Text 임베딩 v2 모델을 제공합니다. 이 모델을 우리의 주요 임베딩 모델로 사용할 것입니다.\n",
    "\n",
    "#### Amazon Bedrock의 Titan Text 임베딩 모델을 호출하기 위한 헬퍼 메서드\n",
    "Amazon Titan Text v2 임베딩 모델을 호출하기 위한 Python 헬퍼 메서드를 생성합니다. 우리는 `df_sample` 데이터 프레임을 업데이트하고 여기에 `embedding`이라는 새로운 열을 추가할 것입니다. 이 셀이 실행되면, 우리의 데이터 프레임은 OpenSearch에 로드될 준비가 됩니다. 실행을 완료하는 데 몇 분 정도 걸릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8cf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# 외부 종속성:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{region}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def add_embeddings_to_df(df, text_column):\n",
    "\n",
    "    # 임베딩을 저장할 빈 목록 만들기\n",
    "    embeddings = []\n",
    "\n",
    "    # 지정된 열의 텍스트를 반복합니다.\n",
    "    for text in df[text_column]:\n",
    "        embedding = embed_phrase(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    # 임베딩을 데이터 프레임에 새 열로 추가합니다.\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_phrase( text ):\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"  # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # 요청 페이로드 준비\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # 응답에서 임베딩 추출\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "\n",
    "    # 목록에 임베딩을 추가합니다.\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "df_sample = add_embeddings_to_df(df_sample, 'description')\n",
    "\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c8425",
   "metadata": {},
   "source": [
    "#### 간단한 입력 텍스트의 임베딩을 생성해 봅시다\n",
    "이것이 부동소수점 숫자의 배열임을 볼 수 있습니다. 인간의 눈/뇌로는 이해하기 어렵지만, 이 숫자 배열은 텍스트의 의미와 지식을 포착하며, 이후 두 개의 다른 텍스트 블록을 비교하는 데 사용될 수 있습니다. 이 방법은 우리의 쿼리를 벡터 표현으로 변환하는 데 사용될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43874004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 입력 텍스트에 대한 벡터 임베딩 만들기\n",
    "#칠면조 가슴살과 잘 어울리는 와인은?\n",
    "input_text = \"A wine that pairs well with turkey breast?\"   \n",
    "\n",
    "embedding = embed_phrase(input_text)\n",
    "\n",
    "#텍스트 출력 및 임베딩\n",
    "print(f\"{input_text=}\")\n",
    "\n",
    "#1024 차원 벡터의 처음 10개 차원만 인쇄합니다. \n",
    "print(f\"{embedding[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717849f2",
   "metadata": {},
   "source": [
    "## 7. Amazon OpenSearch Service에 인덱스 생성\n",
    "이전에 2-3개의 필드로 인덱스를 생성했던 것과 달리, 이번에는 여러 필드로 인덱스를 정의할 것입니다: `description` 필드의 벡터화와 데이터셋 내의 다른 모든 필드들을 포함합니다.\n",
    "\n",
    "인덱스를 생성하기 위해, 먼저 JSON으로 인덱스를 정의한 다음, 앞서 초기화한 aos_client 연결을 사용하여 OpenSearch에 인덱스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"wine_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf4fcb",
   "metadata": {},
   "source": [
    "위의 인덱스 정의를 사용하여 이제 Amazon OpenSearch에 인덱스를 생성해야 합니다. 이 셀을 실행하면 이미 이 노트북을 실행한 경우에도 인덱스를 다시 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"wine_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da3296",
   "metadata": {},
   "source": [
    "생성된 인덱스 정보를 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88682b",
   "metadata": {},
   "source": [
    "## 8. 원본 데이터를 인덱스에 로드\n",
    "다음으로, 방금 생성한 인덱스에 와인 리뷰 데이터와 임베딩을 로드하겠습니다. 우리의 임베딩을 `description_vector` 필드에 저장할 것임을 주목하세요. 이 필드는 나중에 KNN 검색에 사용될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, record in tqdm(df_sample.iterrows()):\n",
    "    body={\"description_vector\": record['embedding'], \n",
    "           \"description\": record[\"description\"],\n",
    "           \"points\":record[\"points\"],\n",
    "           \"variety\":record[\"variety\"],\n",
    "           \"country\":record[\"country\"],\n",
    "           \"designation\":record[\"designation\"],\n",
    "           \"winery\":record[\"winery\"],\n",
    "          \"wine_name\":record[\"title\"]\n",
    "         }\n",
    "    aos_client.index(index=index_name, body=body)\n",
    "print(\"Records loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a03b0d",
   "metadata": {},
   "source": [
    "로드를 검증하기 위해, 인덱스 내의 문서 수를 조회해 보겠습니다. 인덱스에는 300개의 히트(또는 이전 샘플링에서 지정한 만큼의 수)가 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3a5e3",
   "metadata": {},
   "source": [
    "## 9. \"의미론적 검색(Semantic Search)\"으로 벡터 검색\n",
    "\n",
    "이제 요청된 설명과 가장 잘 일치하는 와인 리뷰를 찾기 위한 검색 쿼리를 실행하는 헬퍼 함수를 정의할 수 있습니다. `retrieve_opensearch_with_semantic_search` 함수는 검색 문구를 임베딩하고, 가장 가까운 벡터와 일치하는 인덱스를 검색한 후, 상위 결과를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    search_vector = embed_phrase(phrase)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"description_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"description\":entry['_source']['description'],\n",
    "            \"winery\":entry['_source']['winery'],\n",
    "            \"points\":entry['_source']['points'],\n",
    "            \"designation\":entry['_source']['designation'],\n",
    "            \"country\":entry['_source']['country'],\n",
    "            \"variety\":entry['_source']['variety'],\n",
    "            \"wine_name\":entry['_source']['wine_name'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025eb90",
   "metadata": {},
   "source": [
    "샘플 질문을 사용하여 의미론적 검색으로 유사한 레코드를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#스테이크와 잘 어울리는 최고의 호주 와인은?\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"     \n",
    "\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_wine)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72fe5b",
   "metadata": {},
   "source": [
    "## 10. Amazon Bedrock - Anthropic Claude Sonnet 모델을 호출하는 메서드 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aff961",
   "metadata": {},
   "source": [
    "이제 사용자의 질문에 답변하기 위해 LLM을 호출하는 함수를 정의할 것입니다. LLM은 일반적인 목적의 데이터로 훈련되었기 때문에, 여러분의 와인 리뷰 지식을 가지고 있지 않을 수 있습니다.<br>\n",
    "답변할 수 있을지라도, 그것이 여러분의 비즈니스가 선호하는 답변이 아닐 수 있습니다.<br>\n",
    "예를 들어, `여러분의 경우에는 재고가 없는 와인을 추천하지 않기를 원할 것입니다.`\n",
    "\n",
    "따라서 추천은 여러분의 컬렉션, <u>`즉 우리가 로드한 300개의 리뷰된 와인들 중 하나여야 합니다.`</u>\n",
    "\n",
    "이 함수를 정의한 후, 우리는 와인 리뷰 데이터 없이 LLM이 질문에 어떻게 답변하는지 보기 위해 이를 호출할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Bedrock Runtime 클라이언트 생성\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "    # Claude 3 Sonnet의 모델 ID 설정하기\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # 기본 요청 페이로드로 모델 호출하기\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # 응답 본문 디코딩\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "    # 모델의 페이로드 준비\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4c605",
   "metadata": {},
   "source": [
    "와인 추천에 대해 생성된 결과를 확인해 봅시다. 이는 우리가 재고로 가지고 있는 와인 중 하나가 아닐 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "    \n",
    "    #Claude 모델에는 두 부분으로 구성된 프롬프트가 있습니다. \n",
    "    #시스템 프롬프트는 모델에 어떤 역할을 수행할지 안내합니다.\n",
    "    system_prompt = (\n",
    "                    #소믈리에는 와인에 대한 방대한 지식을 바탕으로 사람들이 즐길 수 있는 훌륭한 추천을 하는 사람입니다.\n",
    "                    #그리고 와인 이름은 영문으로, 와인 설명은 한글로 답해주세요.\n",
    "                    f\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\" \n",
    "                    f\"And please answer the wine name in English and the wine description in Korean.\"\n",
    "    )\n",
    "    \n",
    "    #사용자 프롬프트는 모델이 질문에 답하기 위해 참조해야 하는 컨텍스트가 있는 엔지니어 프롬프트입니다.\n",
    "    user_prompt = (\n",
    "        #소믈리에는 와인 품종과 원산지를 반드시 기재해야 합니다,\n",
    "        #그리고 고객 질문과 관련된 다채로운 설명을 제공합니다.\n",
    "        #고객 질문입니다: {question}\n",
    "        #답변 마지막에 와인 이름을 새 줄에 와인 이름 형식으로 입력하세요: <wine name>        \n",
    "        f\" As a sommelier, you must include the wine variety, the country of origin, \"                                           \n",
    "        f\"and a colorful description relating to the customer question.\"                                                           \n",
    "        f\"\\n Customer question: {question}\"                                                                                        \n",
    "        f\"\\n Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"            \n",
    "    )\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "#스테이크와 잘 어울리는 최고의 호주 와인은?\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"     \n",
    "\n",
    "#RAG가 없는 LLM의 추천 와인: {query_llm_without_rag(question_on_와인)}\n",
    "print(f\"The recommened wine from LLM without RAG: \\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba65cc",
   "metadata": {},
   "source": [
    "#### 환각(hallucination) 테스트\n",
    "마지막 줄에서 와인 이름을 복사하여 아래의 질문 변수에 붙여넣어, 우리 재고에 이 와인이 있는지 확인해 봅시다. 반환된 와인 목록을 검토해 주세요. 포르투갈 와인일 수는 있지만, 모델이 추천한 것과 정확히 일치하지 않을 수 있습니다. 이를 환각이라고 부릅니다. 모델이 일반적인 지식을 바탕으로 임의의 추천을 했기 때문입니다.\n",
    "\n",
    "__참고:__ 위 모델이 추천한 와인 이름이 아래 `wine_name` 변수에 보이지 않는다면, 추천된 와인이 우리의 OpenSearch 인덱스에 없다는 것을 확인할 수 있도록 이를 교체해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff031f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine_name = \"Penfolds Bin 389 Kalimna Shiraz\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(wine_name)\n",
    "\n",
    "#리뷰에서 와인 기록과 일치하는 와인\n",
    "print(\"Matching wine records in our reviews:\")      \n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d3a21c",
   "metadata": {},
   "source": [
    "## 11. 검색 증강 생성(Retrieval Augmented Generation)\n",
    "---\n",
    "LLM의 환각 문제를 해결하기 위해, 우리는 LLM에 더 많은 컨텍스트를 제공할 수 있습니다. 이를 통해 LLM은 컨텍스트 정보를 사용하여 모델을 미세 조정하고 사실에 기반한 결과를 생성할 수 있습니다. RAG는 LLM 환각 문제에 대한 해결책 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbd737",
   "metadata": {},
   "source": [
    "#### OpenSearch 검색 결과를 사용하여 LLM을 위한 프롬프트 생성 (RAG)\n",
    "\n",
    "우리는 Anthropic Claude Sonnet 3 모델을 원샷 프롬프팅 기법과 함께 사용할 것입니다. 모델에 대한 지시사항 내에서, 우리는 샘플 와인 리뷰와 모델이 사용자의 질문에 답변하는 방법을 제공할 것입니다. 프롬프트의 끝에는 OpenSearch에서 검색한 와인 리뷰가 모델이 사용할 수 있도록 포함될 것입니다.\n",
    "\n",
    "모델에 쿼리하기 전에, 아래의 `generate_rag_based_system_prompt` 함수를 사용하여 사용자 프롬프트를 구성합니다. 이 함수는 OpenSearch 클러스터에서 일치하는 와인을 검색하기 위한 입력 문자열을 받아, LLM을 위한 사용자 프롬프트를 작성합니다.\n",
    "\n",
    "시스템 프롬프트는 LLM이 수행할 역할을 정의합니다.\n",
    "\n",
    "사용자 프롬프트는 LLM 모델이 사용자의 질문에 답변하기 위해 사용하는 지시사항과 컨텍스트 정보를 포함합니다.\n",
    "\n",
    "프롬프트는 다음과 같은 형식을 가집니다:\n",
    "\n",
    "**시스템 프롬프트:**\n",
    "\n",
    "```\n",
    "당신은 와인에 대한 광범위한 지식을 사용하여 사람들이 즐길 수 있는 훌륭한 추천을 하는 소믈리에입니다.\n",
    "```\n",
    "\n",
    "**사용자 프롬프트**\n",
    "```\n",
    "소믈리에로서, 당신은 와인 품종, 원산지 국가, 그리고 사용자의 질문과 관련된 생생한 설명을 포함해야 합니다.\n",
    "\n",
    "Data:{'description': '이 향기로운 화이트 와인은 강렬하고 크리미한 층의 핵과류와 바닐라 맛으로 춤추며, 처음부터 끝까지 활기차고 균형 잡힌 맛을 유지합니다. 풍부한 과일은 나파 밸리의 비교적 서늘한 오크 놀 섹션에서 재배됩니다. 시간이 지나고 글라스에서 더욱 발전할 것입니다.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation: 당신을 위한 훌륭한 와인이 있습니다. 미국 나파 밸리의 오크 놀 섹션에 있는 Darioush 와이너리의 드라이하고 중간 바디감의 화이트 와인입니다. 바닐라와 오크 풍미가 있습니다. 와인 스펙테이터에서 92점을 받았습니다.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "사용자의 질문 그대로\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3926d8b",
   "metadata": {},
   "source": [
    "### 프롬프트 패키징 및 LLM 쿼리\n",
    "프롬프트로 LLM을 쿼리하기 위한 최종 함수를 만들 것입니다. `query_llm_with_rag`는 RAG에서 LLM을 호출하는 함수입니다.\n",
    "\n",
    "`query_llm_with_rag`는 이 모듈에서 우리가 수행한 모든 것을 결합합니다. 다음과 같은 모든 작업을 수행합니다:\n",
    "- \"description vector\"를 사용하여 의미론적 검색으로 OpenSearch 인덱스에서 관련 와인을 검색합니다.\n",
    "- 검색 결과로부터 LLM 프롬프트를 생성합니다.\n",
    "- RAG를 사용하여 LLM에 응답을 쿼리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(user_question)\n",
    "    #\"{'description': '이 향기로운 화이트 와인은 강렬하고 크리미한 층의 핵과류와 바닐라 맛으로 춤을 추며, 처음부터 끝까지 활기차고 균형 잡힌 맛을 유지합니다. 풍부한 과일은 나파 밸리의 비교적 서늘한 오크 놀 섹션에서 재배됩니다. 이 와인은 시간이 지나고 글라스에서 더욱 발전할 것입니다.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "\n",
    "    #\"당신을 위한 훌륭한 와인이 있습니다. 미국 나파 밸리의 오크 놀 섹션에 있는 Darioush 와이너리의 드라이하고 중간 바디감의 화이트 와인입니다. 바닐라와 오크 풍미가 있습니다. 와인 스펙테이터에서 92점을 받았습니다.\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt= (\n",
    "        #와인에 대한 방대한 지식을 활용하여 사람들이 즐길 수 있는 훌륭한 추천을 하는 소믈리에입니다.\n",
    "        #그리고 와인 이름은 영문으로, 와인 설명은 한글로 답해주세요.\n",
    "        f\"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy\"                                \n",
    "        f\"And please answer the wine name in English and the wine description in Korean.\"\n",
    "\n",
    "    )\n",
    "    user_prompt = (\n",
    "        #소믈리에는 와인 품종, 원산지 및 사용자 질문과 관련된 다채로운 설명을 포함해야 합니다. '와인 데이터' 섹션에서만 고객 질문과 가장 잘 어울리는 와인을 선택해야 합니다. 제공된 와인 데이터 외에 다른 와인을 제안하지 마세요. 고객 질문에 가장 적합하지 않다고 해서 반드시 최고 등급의 와인을 선택할 필요는 없습니다.\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not suggest anything outside of the wine data provided. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Wine data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents} \\n\"\n",
    "        f\"Customer Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd128d",
   "metadata": {},
   "source": [
    "#### 마지막으로 함수를 호출하여 와인 추천을 받아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#스테이크와 잘 어울리는 최고의 호주 와인은?\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"     \n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "#위의 권장 사항에 대해 검색된 문서는 다음과 같습니다.\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058e50f",
   "metadata": {},
   "source": [
    "#### 이탈리아 와인으로 변경해 보겠습니다. 일치하는 결과가 나올 것입니다.\n",
    "동일한 메서드를 다시 호출하여 카탈로그에 이탈리아 와인이 있는지 확인하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#스테이크와 잘 어울리는 최고의 이탈리아 와인은?\n",
    "question_on_wine=\"Best Italian wine that goes great with steak?\"        \n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "#위의 권장 사항에 대해 검색된 문서는 다음과 같습니다.\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e3d0c",
   "metadata": {},
   "source": [
    "스테이크와 잘 어울리는 호주 와인을 요청했는데 우리 컬렉션에는 그런 와인이 없다는 것을 눈치채셨을 것입니다. <br>\n",
    "따라서 모델은 정중하게 다음과 같이 변명합니다.<br> \n",
    "질문을 바꿔서 LLM이 셀렉트 리스트에서 귀하의 질문에 가장 잘 맞는 와인을 추천하는 방법을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158288ed",
   "metadata": {},
   "source": [
    "### 결론\n",
    "이 실습에서는 간단한 와인 추천 챗봇을 구축했습니다. 이 실습에서는 데이터에 대한 벡터 임베딩을 생성하기 위해 Amazon Bedrock titan v2 모델을 사용했습니다. 그런 다음 이 데이터를 `description_vector` 필드를 사용하여 OpenSearch 인덱스에 로드했습니다. 검색 시에는 Amazon Titan v2 모델을 다시 사용하여 쿼리 질문을 벡터 임베딩으로 변환하고 시맨틱 검색을 사용하여 결과를 검색했습니다. 그런 다음 이 결과를 Anthropic Claude Sonnet 3 모델에 전달하여 카탈로그 내에서 와인을 추천할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e485b",
   "metadata": {},
   "source": [
    "## 실습 완료 - 이제 실습 지침 섹션으로 돌아갈 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad865",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
