{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb05169",
   "metadata": {},
   "source": [
    "# OpenSearch 신경망(Neural) 플러그인을 이용한 고급 RAG\n",
    "이 실습에서는 PDF 문서를 처리하는 고급 RAG 아키텍처를 살펴볼 것입니다. 먼저 PDF에서 텍스트를 추출한 다음, 재귀적 문자 청킹(Recursive Character Chunking) 기법을 사용하여 청크로 나눕니다. OpenSearch 신경망 플러그인을 사용하여 데이터 수집 시 이러한 청크를 벡터로 변환합니다. 데이터가 수집되면 신경망 플러그인을 사용하여 의미론적으로 데이터를 검색합니다. 반환된 결과를 사용하여 답변을 생성하기 위한 프롬프트를 엔지니어링합니다.\n",
    "\n",
    "우리가 사용할 PDF는 Amazon이 발행한 2023년 연간 보고서입니다. 이 문서에는 재무 데이터, 성과, 회사가 직면한 위험 및 미래에 대한 지침이 포함되어 있습니다. 우리는 이 문서에서 질문에 답할 수 있는 재무 분석가 보조 봇 역할을 할 것입니다.\n",
    "\n",
    "먼저 적절한 라이브러리를 로드하는 것부터 시작하겠습니다.\n",
    "\n",
    "## 1. 사전 요구 사항 설치 및 변수 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fb6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community pypdf langchain_experimental --quiet\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install --upgrade --quiet  boto3\n",
    "!pip install pdfminer.six --quiet\n",
    "!pip install opensearch-py --quiet\n",
    "!pip install \"unstructured[all-docs]\" --quiet\n",
    "!pip install pdf2image --quiet\n",
    "!pip install -qU langchain-aws --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install opensearch-py-ml --quiet\n",
    "!pip install requests_aws4auth --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1915eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests \n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd7142",
   "metadata": {},
   "source": [
    "### CloudFormation 스택 출력에서 변수 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87760ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86734",
   "metadata": {},
   "source": [
    "## 2. PDF에서 텍스트 추출 및 청킹\n",
    "문서를 청크로 나누는 가장 간단한 방법은 길이로 나누는 것이지만, 의미를 잃지 않도록 단락이나 줄을 함께 유지하는 것이 중요합니다. 우리는 LangChain 라이브러리의 재귀적 문자 텍스트 분할기(RecursiveCharacterTextSplitter)를 사용할 것입니다. 이 분할기는 데이터를 길이로 분할하면서도 가능한 한 줄과 단락을 함께 유지하는 방법을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a82f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this method would split the text into chunks by paragraph, line boundary and keeping chunk \n",
    "# size as close to 1000 characters, it will also overlap the text between chunks if it were to \n",
    "# split line or paragraph in the middle.\n",
    "\n",
    "def recursive_character_chunking(text): \n",
    "    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=200, #number of characters that can overlap with previous chunk\n",
    "        length_function=len,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.create_documents(text)#From the loaded PDF\n",
    "    \n",
    "    return docs #return the index to be cached by the client app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b894d",
   "metadata": {},
   "source": [
    "### PDF 파일 로드 및 파싱\n",
    "이제 LangChain 라이브러리의 PyPDFLoader를 사용하여 PDF를 로드하고 텍스트를 추출하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD A PDF DOCUMENT\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Amazon-com-Inc-2023-Annual-Report.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#print(documents)a\n",
    "\n",
    "texts = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    texts += document.page_content.replace(r'\\\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8187e",
   "metadata": {},
   "source": [
    "이제 재귀적 문자 청킹(Recursive Character Chunking) 기법을 사용하여 로드된 PDF 텍스트를 청크로 나누겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LETS RECURISIVE CHUNK IT\n",
    "docs = recursive_character_chunking([texts])\n",
    "\n",
    "# the method prints chunks\n",
    "def print_chunks(data):\n",
    "    #Let's print the chunks -- notice the overlap between chunk 3 and 4\n",
    "    i = 1\n",
    "    for doc in data:\n",
    "        print(f\"---------START OF CHUNK {i}------\")\n",
    "        print(f\"{doc.page_content}\")\n",
    "        print(f\"---------END OF CHUNK {i}------\\n\\n\")\n",
    "        i+=1\n",
    "\n",
    "#Let's print first 5 chunks.\n",
    "print_chunks(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f4a6d",
   "metadata": {},
   "source": [
    "## 3. OpenSearch 도메인과 연결 생성\n",
    "다음으로, Python API를 사용하여 OpenSearch 도메인과의 연결을 설정하겠습니다.\n",
    "\n",
    "#### 참고: \n",
    "이 실습 중 어느 시점에서든 **_The security token included in the request is expired._** 라는 실패 메시지가 표시되면, 이 셀을 다시 실행하여 해결할 수 있습니다. 이 셀은 실습의 나머지 부분에 필요한 보안 자격 증명을 새로 고칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59694500",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "host = f'https://{aos_host}/'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing some variables that we will use later.\n",
    "\n",
    "connector_id = \"\"\n",
    "model_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449368b",
   "metadata": {},
   "source": [
    "## 4. Amazon Bedrock Titan Text Embedding v2에 대한 ML 커넥터(ML Connector) 생성 및 배포\n",
    "\n",
    "OpenSearch에서 ML 커넥터를 구성하기 위해 Amazon SageMaker Notebook IAM 역할을 사용할 것입니다. 이 IAM 역할은 BedrockInference IAM 역할을 OpenSearch에 전달할 수 있는 권한을 가지고 있습니다. 그러면 OpenSearch는 BedrockInference IAM 역할을 사용하여 Bedrock 모델을 호출할 수 있게 됩니다.\n",
    "\n",
    "다음 셀은 SageMaker Notebook IAM 역할을 사용하여 ML 커넥터를 생성합니다. 이 셀은 Amazon Bedrock Titan Text embedding v2 모델과 함께 OpenSearch 원격 ML 커넥터를 생성합니다. 다음 셀은 ML 커넥터 구성을 정의합니다.\n",
    "\n",
    "#### 중요한 전제 조건\n",
    "실습 지침 섹션의 단계에 따라 SageMaker 노트북 역할을 OpenSearch의 `ml_full_access` 역할에 매핑해야 합니다. 그렇지 않은 경우, 실습 지침을 참조하여 **권한 설정 > OpenSearch 권한** 섹션을 완료하세요. 이 작업을 수행하지 않으면 다음 셀에서 오류가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "\n",
    "if not connector_id:\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan text embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "           ## USING AMAZON BEDROCK TITAN EMBED TEXT MODEL\n",
    "        \"model\": \"amazon.titan-embed-text-v2:0\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "         \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "         \"post_process_function\": \"connector.post_process.bedrock.embedding\"}\n",
    "       ]\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    if r.status_code == 403:\n",
    "        print(\"Permission Error: Please make sure you have mapped the NB role to ml_full_access role in OpenSearch dashboard. Follow permission section in lab instructions.\")\n",
    "        print(r.text)\n",
    "    else:\n",
    "        connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "        print(r.text)\n",
    "else:\n",
    "    print(f\"Connector already exists - {connector_id}\")\n",
    "    \n",
    "connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89985115",
   "metadata": {},
   "source": [
    "ML 커넥터가 정의되면, 모델을 등록하고 배포해야 합니다. 다음 두 셀은 각각 모델 연결을 등록하고 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "if not model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Bedrock Titan embeddings model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"Bedrock Titan text embeddings model\",\n",
    "    \"connector_id\": connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - model already exists\")\n",
    "print(\"Model registered under model_id: \"+model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7676c",
   "metadata": {},
   "source": [
    "마지막으로 원격 추론을 가능하게 하는 모델을 배포할 것입니다. 이 모델은 텍스트를 임베딩으로 변환하는 데 사용할 모델입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14098c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "path = '_plugins/_ml/models/'+model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a896856",
   "metadata": {},
   "source": [
    "### 배포된 모델이 제대로 작동하는지 확인하기 위한 테스트 임베딩 생성\n",
    "\n",
    "OpenSearch ML Commons 플러그인 Python 클라이언트의 메서드인 `ml_client.generate_embedding`을 사용하여 임베딩 모델을 호출하고 임베딩을 생성해 보겠습니다.\n",
    "\n",
    "**사전 요구 사항**: 실습 지침의 _**권한 설정 > Amazon Bedrock 권한**_ 섹션을 따라 Amazon Bedrock 모델 액세스를 설정했는지 확인하세요. 이 작업을 수행하지 않았다면, 다음 셀을 실행할 때 인증 예외가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model working\n",
    "\n",
    "input_sentences = [\"What an easy way to create embeddings\"]\n",
    "embedding_output = ml_client.generate_embedding(f\"{model_id}\", input_sentences)\n",
    "embed = embedding_output['inference_results'][0]['output'][0]['data']\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67095fe2",
   "metadata": {},
   "source": [
    "## 5. 수집 파이프라인(Ingest Pipeline) 생성\n",
    "Amazon Bedrock Titan Text 임베딩 모델을 호출하여 텍스트 청크 레코드의 `doc_chunk_text` 필드를 벡터 임베딩으로 변환하는 수집 파이프라인을 만들어 보겠습니다. 수집 파이프라인은 OpenSearch의 기능으로, 데이터 수집 시 수행할 특정 작업을 정의할 수 있습니다. 정적 필드 추가, 기존 필드 수정, 또는 원격 모델을 호출하여 추론을 얻고 추론 출력을 인덱싱된 레코드/문서와 함께 저장하는 등의 간단한 처리를 수행할 수 있습니다. 우리의 경우 추론 출력은 벡터 임베딩입니다.\n",
    "\n",
    "다음 수집 파이프라인은 우리의 원격 모델을 호출하여 청크 텍스트 `doc_chunk_text` 필드를 벡터로 변환하고 이를 `doc_chunk_embedding`이라는 필드에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_ingest/pipeline/amazon-report-ingest-pipeline\"\n",
    "url = f\"{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"An Index of Amazon annual report\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": f\"{model_id}\",\n",
    "        \"field_map\": {\n",
    "          \"doc_chunk_text\": \"doc_chunk_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "aos_client.ingest.put_pipeline(id=\"amazon-report-ingest-pipeline\", body=payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ad347",
   "metadata": {},
   "source": [
    "## 6. Amazon OpenSearch Service에 인덱스 생성\n",
    "이제 PDF 문서 지식 베이스를 위한 인덱스를 정의하겠습니다. 우리는 두 개의 필드를 정의할 것입니다. 첫 번째는 텍스트 청크이고 두 번째는 벡터 임베딩입니다. 벡터 임베딩을 위해 `FAISS` 엔진과 `HNSW`를 알고리즘/방법으로 사용할 것입니다. 다른 매개변수에 대해서는 적절한 기본값을 사용하겠습니다. 위에서 생성한 파이프라인을 인덱스로 수집되는 데이터의 기본 파이프라인으로 제공한다는 점에 주목하세요. 이렇게 하면 문서가 인덱스에 수집되기 전에 파이프라인을 거치게 됩니다.\n",
    "\n",
    "인덱스를 생성하기 위해, 먼저 JSON으로 인덱스를 정의한 다음, 앞서 초기화한 aos_client 연결을 사용하여 OpenSearch에 인덱스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEFINE INDEX JSON\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"default_pipeline\": \"amazon-report-ingest-pipeline\", \n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "             \"doc_chunk_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "           \"doc_chunk_embedding\": {\n",
    "               \"type\": \"knn_vector\",\n",
    "               \"dimension\": 1024,\n",
    "               \"method\": {\n",
    "                   \"name\": \"hnsw\",\n",
    "                   \"space_type\": \"l2\",\n",
    "                   \"engine\": \"faiss\",\n",
    "                   \"parameters\": {\n",
    "                       \"ef_construction\": 256,\n",
    "                       \"m\": 48\n",
    "                   }\n",
    "               }\n",
    "           }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e30846",
   "metadata": {},
   "source": [
    "위의 인덱스 정의를 사용하여 이제 Amazon OpenSearch Service에 인덱스를 생성해야 합니다. 이 셀을 실행하면 이전에 이 노트북을 실행한 적이 있는 경우 인덱스를 다시 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"amazon_report_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585828d3",
   "metadata": {},
   "source": [
    "생성된 인덱스 정보를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e700a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124671d5",
   "metadata": {},
   "source": [
    "## 7. 인덱스에 원시 데이터 로드\n",
    "다음으로, 방금 생성한 인덱스에 청크로 나눈 텍스트 데이터와 그 임베딩을 로드하겠습니다. 우리의 임베딩을 `doc_chunk_embedding` 필드에 저장할 것이며, 이는 나중에 KNN 검색에 사용될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "batch = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": index_name } })\n",
    "body_ = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(docs), force_tty = True) as bar:\n",
    "    for doc in docs:\n",
    "\n",
    "        payload={\n",
    "           \"doc_chunk_text\": doc.page_content\n",
    "        }\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        if(cnt == 100):\n",
    "            \n",
    "            response = aos_client.bulk(\n",
    "                                index = index_name,\n",
    "                                 body = body_)\n",
    "            \n",
    "\n",
    "            cnt = 0\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8d61a",
   "metadata": {},
   "source": [
    "로드를 검증하기 위해, 인덱스에 있는 문서 수를 조회해 보겠습니다. 인덱스에는 약 400개의 문서가 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c227db",
   "metadata": {},
   "source": [
    "## 8. \"의미론적 검색(Semantic Search)\"으로 벡터 검색\n",
    "\n",
    "이제 사용자의 질문과 의미론적으로 일치하는 청크를 찾기 위한 검색 쿼리를 실행하는 헬퍼 함수를 정의할 수 있습니다. `retrieve_opensearch_with_semantic_search`는 신경망 쿼리를 사용하며, 이는 쿼리 텍스트와 모델 ID를 받아 쿼리 텍스트를 임베딩으로 변환한 후 근사 이웃 검색(즉, 의미론적 검색)을 실행합니다. OpenSearch가 반환할 결과의 개수를 지정하기 위해 K를 매개변수로 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d499e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"doc_chunk_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{model_id}\",\n",
    "            \"k\": n\n",
    "          }\n",
    "        }\n",
    "      }    \n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bb79e",
   "metadata": {},
   "source": [
    "### 샘플 질문으로 의미론적 검색을 사용하여 유사한 레코드 가져오기\n",
    "\n",
    "우리의 청크 데이터에서 답변할 수 있다고 생각되는 질문을 할 것입니다. OpenSearch는 질문과 일치하는 상위 결과를 반환할 것입니다. 반환된 청크들이 질문의 주제에 대해 이야기하고 있음을 알 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86275ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs = \"What was the operating income difference between 2022 and 2023?\" # \"2022년과 2023년 사이의 영업 이익 차이는 얼마였습니까?\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_docs)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f3b0a",
   "metadata": {},
   "source": [
    "## 9. Amazon Bedrock - Anthropic Claude Sonnet 모델 호출 방법 준비\n",
    "이제 사용자의 질문에 답변하기 위해 LLM을 호출하는 함수를 정의하겠습니다. LLM은 PDF가 생성되기 전에 일반적인 목적의 데이터로 훈련되었을 가능성이 높기 때문에, PDF 파일에 숨겨진 지식을 가지고 있지 않을 수 있습니다. 답변할 수 있더라도 귀하의 비즈니스가 선호하는 답변이 아닐 수 있습니다. 따라서 답변은 우리가 프롬프트에 전달한 데이터를 참조해야 합니다.\n",
    "\n",
    "이 함수를 정의한 후, LLM이 PDF 청크 데이터에서 어떻게 질문에 답변하는지 확인하기 위해 호출해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d844b",
   "metadata": {},
   "source": [
    "## 10. 검색 증강 생성 (Retrieval Augmented Generation)\n",
    "\n",
    "### OpenSearch 검색 결과를 사용하여 LLM용 프롬프트 생성 (RAG)\n",
    "\n",
    "우리는 Anthropic Sonnet 모델을 원샷 프롬프팅 기법과 함께 사용할 것입니다. 프롬프트 내의 모델 지시사항 안에서, 프롬프트 끝에 OpenSearch에서 검색한 청크를 제공하여 모델이 사용자 질문에 답변할 때 참조하도록 할 것입니다.\n",
    "\n",
    "모델에 쿼리하기 전에, 아래의 `generate_rag_based_system_prompt` 함수를 사용하여 사용자 프롬프트를 구성합니다. 이 함수는 입력 문자열을 받아 OpenSearch 클러스터에서 일치하는 청크를 검색한 다음, LLM을 위한 사용자 프롬프트를 작성합니다.\n",
    "\n",
    "시스템 프롬프트는 LLM이 수행할 역할을 정의합니다.\n",
    "\n",
    "사용자 프롬프트는 LLM 모델이 사용자의 질문에 답변하기 위해 사용하는 지시사항과 컨텍스트 정보를 포함합니다.\n",
    "\n",
    "프롬프트는 다음 형식을 따릅니다:\n",
    "\n",
    "**시스템 프롬프트:**\n",
    "\n",
    "```\n",
    "당신은 재무 문서에서 제공된 텍스트를 분석하고 사용자 질문에 답변하는 재무 보고서 분석 봇입니다.\n",
    "```\n",
    "\n",
    "**사용자 프롬프트**\n",
    "```\n",
    "재무 보고서 분석가로서 제공된 DOCS_DATA에서 질문에 대한 답변을 해주세요. DOCS_DATA에서 답변을 찾을 수 없다면, \"주어진 정보로는 이 질문에 답변할 수 없습니다\"라고 말해주세요. DOCS_DATA에서 답변을 얻었다면 그 사실을 언급할 필요는 없습니다.\n",
    "\n",
    "다음은 DOCS_DATA이며, 이후에 답변할 사용자의 질문이 제시될 것입니다.\n",
    "\n",
    "DOCS_DATA: {retrieved_documents}\n",
    "\n",
    "사용자 질문: <사용자 질문>\n",
    "```\n",
    "\n",
    "우리는 이 모듈에서 수행한 모든 작업을 결합하는 `query_llm_with_rag` 메서드를 정의합니다. 이 메서드는 다음과 같은 작업을 수행합니다:\n",
    "- 관련 청크를 찾기 위해 의미론적 검색으로 OpenSearch 인덱스를 검색합니다.\n",
    "- 검색 결과로부터 LLM 프롬프트를 생성합니다.\n",
    "- RAG를 사용하여 LLM에 쿼리하여 응답을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa520f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question, n=3):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(user_question,n)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152590e",
   "metadata": {},
   "source": [
    "### RAG를 사용하여 질문에 답변하기\n",
    "PDF 텍스트 데이터에서 답변할 수 있는 질문을 해봅시다. 연간 보고서에서 답변할 수 있는 다른 재무 또는 일반적인 회사 성과 관련 질문으로 변경해 보시기를 권장합니다. 이 간단한 아키텍처가 연간 재무 보고서와 같은 복잡한 문서를 분석하는 데 정말 좋은 방법을 제공한다는 것을 알 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What was the revenue for amazon advertisement in 2023?\" # \"2023년 Amazon 광고 수익은 얼마였습니까?\"\n",
    "recommendation = query_llm_with_rag(question_on_docs)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350cf368",
   "metadata": {},
   "source": [
    "## 11. 하이브리드 검색 - 고급 RAG\n",
    "다음 섹션에서는 특정 상황에서 하이브리드 검색이 어떻게 도움이 될 수 있는지 살펴보겠습니다.\n",
    "\n",
    "Amazon 보고서에서 **지식 베이스(Knowledge Base)** 는 실시간 쿼리에 답변하기 위한 RAG 또는 검색 증강 생성 기반 기술로 언급됩니다.\n",
    "\n",
    "RAG를 위한 AWS 제품을 검색하는 예시로 다음 질문을 사용해 보겠습니다. 임베딩 모델이 RAG의 의미론적 의미를 알지 못할 수 있기 때문에, 사용자 쿼리의 다른 용어를 사용하여 청크의 정보와 일치시킬 것입니다. 이로 인해 LLM이 우리의 질문에 답변하지 못할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\" # \"AWS는 검색 증강 생성(retrieval augmented generation)을 위해 어떤 서비스를 제공하고 있습니까?\"\n",
    "recommendation = query_llm_with_rag(question_on_docs, n=3)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs, n=3), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22610728",
   "metadata": {},
   "source": [
    "### 이 문제를 어떻게 해결할 수 있을까요?\n",
    "보시다시피 상위 3개의 청크에는 사용자의 질문에 답할 수 있는 정보가 포함되어 있지 않습니다. 이런 경우 벡터 검색은 비슷해 보이는 여러 청크를 찾을 수 있지만, 단순히 유사도 거리만 보고는 답변이 있는 청크를 상위로 반환하지 못할 수 있습니다. 우리는 의미론적 검색을 전통적인 키워드 검색과 결합하여 결과를 보완할 수 있습니다. 이를 **하이브리드 검색**이라고 합니다. 키워드와 의미론적 검색 모두의 결과가 다양한 정규화 및 결합 기법을 통해 병합되어 최종 결과를 생성합니다. 이제 **_phase_results_processors_** 결과 프로세서를 사용하는 검색 파이프라인을 정의해 보겠습니다. 이 프로세서는 2개의 서로 다른 쿼리를 받아 그 결과를 결합합니다. 우리의 경우 키워드 일치에 40%, 의미론적 일치에 60%의 가중치를 부여하고, **조화 평균** 기법을 사용하여 결과를 결합합니다.\n",
    "\n",
    "우리의 쿼리는 이전과 같이 전체 텍스트 검색과 의미론적 검색이 될 것입니다. 우리는 검색 방법을 다시 작성하고 키워드 쿼리를 추가합니다. 이 과정을 `retrieve_opensearch_with_hybrid_search` 메서드로 정의합니다. \"**hybrid**\" 섹션 아래에 이 메서드가 2개의 쿼리 절을 가지고 있음을 주목하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_search/pipeline/hybrid-search-pipeline\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"Hybrid search over amazon financial report\",\n",
    "  \"phase_results_processors\":[\n",
    "      {\n",
    "          \"normalization-processor\":{\n",
    "              \"normalization\": {\n",
    "                  \"technique\": \"l2\"\n",
    "              },\n",
    "              \"combination\": {\n",
    "                  \"technique\": \"harmonic_mean\",\n",
    "                  \"parameters\": {\n",
    "                      \"weights\": [\n",
    "                          0.4, #first query i.e. keywords have 20% weightage\n",
    "                          0.6  #first query i.e. semantic search have 80% weightage\n",
    "                      ]\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_opensearch_with_hybrid_search(phrase, model_id=model_id, bedrock_client=bedrock_client, n=3 ):\n",
    "    osquery={\n",
    "            \"_source\": {\n",
    "                \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "            },\n",
    "          \"size\": n,\n",
    "          \"query\": {\n",
    "            \"hybrid\": {\n",
    "              \"queries\": [\n",
    "                {\n",
    "                  #First query clause that performs keyword search\n",
    "                  \"match\": {\n",
    "                    \"doc_chunk_text\": {\n",
    "                      \"query\": f\"{phrase}\"\n",
    "                    }\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  #Second query clause that performs semantic search\n",
    "                  \"neural\": {\n",
    "                    \"doc_chunk_embedding\": {\n",
    "                      \"query_text\": f\"{phrase}\",\n",
    "                      \"model_id\": f\"{model_id}\",\n",
    "                      \"k\": n\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          }    \n",
    "        }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           search_pipeline=\"hybrid-search-pipeline\",\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"id\":entry['_id'],\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text'],\n",
    "            \"_score\":entry['_score']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def hybrid_query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_opensearch_with_hybrid_search(user_question)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41384fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\" # \"AWS는 검색 증강 생성(retrieval augmented generation)을 위해 어떤 서비스를 제공하고 있습니까?\"\n",
    "recommendation = hybrid_query_llm_with_rag(question_on_docs)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_hybrid_search(question_on_docs, model_id=model_id, bedrock_client=bedrock_client), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3dcb6b",
   "metadata": {},
   "source": [
    "### 하이브리드 검색은 인기 있지만...\n",
    "\n",
    "고객들이 하이브리드 검색을 자주 사용하는 것을 보고 있습니다. 이는 단순히 의미론적 검색만으로는 충분하지 않은 경우에 유용합니다. 특히, 도메인 특화 용어(예: RAG)를 포함한 사용 사례에서는 더욱 그렇습니다. 이러한 용어는 임베딩 모델이 학습 데이터에서 충분히 접하지 못했거나, 회사의 분류 체계에 매우 특정되어 전통적인 의미와는 다르게 해석될 수 있습니다.\n",
    "\n",
    "또한, 의미론적 검색에서 찾은 상위 3개의 결과에 답이 없는 경우일 수도 있습니다. 이번에는 **n=10**, 즉 상위 10개의 결과로 의미론적 검색을 다시 시도해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\" # \"AWS는 검색 증강 생성(retrieval augmented generation)을 위해 어떤 서비스를 제공하고 있습니까?\"\n",
    "\n",
    "#Setting n=10 to retrieve 10 results instead of 3\n",
    "n = 10\n",
    "recommendation = query_llm_with_rag(question_on_docs, n)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_docs, n=n), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e63d1",
   "metadata": {},
   "source": [
    "보시다시피 모델이 질문에 조금 더 잘 답변할 수 있었습니다.\n",
    "\n",
    "## 12. 교차 인코더(Cross-Encoder) 모델을 사용한 재순위화(Re-Ranking)\n",
    "\n",
    "위의 문제는 RAG에 대해 언급하는 텍스트 청크가 상위 3개 결과 안에 들어오도록 결과를 재순위화하는 방법을 찾을 수 있다면 해결할 수 있습니다. 이는 쿼리와 결과 세트를 살펴보며 OpenSearch 결과를 재순위화하는 데 도움을 주는 교차 인코더 모델로 잠재적으로 달성할 수 있습니다.\n",
    "\n",
    "교차 인코더 모델은 OpenSearch에서 지원되며, OpenSearch 서비스에서 이러한 모델 중 두 가지를 사용할 수 있습니다.\n",
    "\n",
    "교차 인코더 모델은 쿼리와 검색 결과를 살펴보고, 쿼리에 답변할 가능성이 있는 결과가 더 높은 순위를 갖도록 재순위화합니다.\n",
    "\n",
    "### 로컬 모델 호스팅을 가능하게 하는 클러스터 설정을 해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56514125",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = b\"\"\"\n",
    "{\"transient\":{\"plugins.ml_commons.only_run_on_ml_node\": false}}\n",
    "\"\"\"\n",
    "aos_client.cluster.put_settings(body=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea76c2",
   "metadata": {},
   "source": [
    "### 교차 인코더 모델 배포\n",
    "OpenSearch의 ML commons 기능을 사용하여 교차 인코더 모델을 로드하겠습니다. 이 모델은 OpenSearch의 데이터 노드 내에서 호스팅됩니다. 이전에는 ML commons를 사용하여 Amazon Bedrock Titan을 호출하는 원격 추론 모델을 배포했습니다. 이번에는 데이터 노드 내에 교차 인코더 모델을 등록하고 배포할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47aec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_id=\"\"\n",
    "\n",
    "path =  \"/_plugins/_ml/model_groups/_register\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"local_model_group\",\n",
    "  \"description\": \"A model group for local models\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "model_group_id = json.loads(r.text)[\"model_group_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85b185",
   "metadata": {},
   "source": [
    "이제 모델을 등록하겠습니다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1825efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_plugins/_ml/models/_register\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"huggingface/cross-encoders/ms-marco-MiniLM-L-6-v2\",\n",
    "  \"version\": \"1.0.2\",\n",
    "  \"model_group_id\": f\"{model_group_id}\",\n",
    "  \"model_format\": \"TORCH_SCRIPT\"\n",
    "}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "rr_model_task_id = json.loads(r.text)[\"task_id\"]\n",
    "\n",
    "rr_model_task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21517e7",
   "metadata": {},
   "source": [
    "다음 셀은 모델 등록이 완료되었는지 확인합니다. **모델이 등록되었다는 메시지가 표시될 때까지 이 셀을 실행해 주세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model_id = \"\"\n",
    "\n",
    "path =  f\"/_plugins/_ml/tasks/{rr_model_task_id}\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.get(url, auth=awsauth, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "task_state = json.loads(r.text)[\"state\"]\n",
    "if task_state == \"COMPLETED\":\n",
    "    rr_model_id = json.loads(r.text)[\"model_id\"]\n",
    "    print(\"TASK COMPLETED SUCCESSFULLY, PLEASE MOVE TO NEXT CELL\")\n",
    "else:\n",
    "    print(\"TASK NOT COMPLETED, PLEASE RE-RUN THIS CELL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33ee8d",
   "metadata": {},
   "source": [
    "다음 코드는 등록된 모델을 배포합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  f\"/_plugins/_ml/models/{rr_model_id}/_deploy\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "rr_model_task_id = json.loads(r.text)[\"task_id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb185ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  f\"/_plugins/_ml/tasks/{rr_model_task_id}\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "r = requests.get(url, auth=awsauth, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)\n",
    "task_state = json.loads(r.text)[\"state\"]\n",
    "if task_state == \"COMPLETED\":\n",
    "    print(\"MODEL DEPLOYMENT SUCCESSFUL, PLEASE MOVE TO THE NEXT CELL\")\n",
    "    rr_model_id = json.loads(r.text)[\"model_id\"]\n",
    "elif task_state == \"FAILED\":\n",
    "    print(\"MODEL DEPLOYMENT FAILED, PLEASE INVESTIGATE THE ERROR\")\n",
    "else:\n",
    "    print(\"MODEL DEPLOYMENT IN PROGRESS, PLEASE RE-RUN THE CELL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab23a5",
   "metadata": {},
   "source": [
    "### 재순위화 모델을 사용하는 검색 파이프라인(Search Pipeline) 생성\n",
    "이제 방금 배포한 재순위화 모델에 검색 결과를 전달하는 검색 파이프라인을 배포하겠습니다. 이 모델은 교차 인코더가 결과 순위를 평가하는 데 사용할 필드의 이름을 필요로 합니다. 이 경우 우리는 `doc_chunk_text`를 필드로 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_search/pipeline/rerank-search-pipeline\"\n",
    "url = f\"https://{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"Pipeline for reranking with cross-encoder model\",\n",
    "    \"response_processors\": [\n",
    "        {\n",
    "            \"rerank\": {\n",
    "                \"ml_opensearch\": {\n",
    "                    \"model_id\": f\"{rr_model_id}\"\n",
    "                },\n",
    "                \"context\": {\n",
    "                    \"document_fields\": [\"doc_chunk_text\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(payload)\n",
    "\n",
    "r = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62127e",
   "metadata": {},
   "source": [
    "의미론적 검색 메서드를 다시 작성해 보겠습니다. 재순위화(rerank) 절을 사용하고 우리의 쿼리를 쿼리 컨텍스트로 제공할 것입니다. 이 쿼리와 검색 파이프라인의 일부로 제공한 `doc_chunk_text`를 통해 교차 인코더는 재순위화를 수행하는 데 필요한 모든 것을 갖게 됩니다. 다음 두 가지 메서드를 정의하겠습니다:\n",
    "\n",
    "`retrieve_opensearch_with_semantic_rerank_search` - 이 메서드는 의미론적 검색을 실행하고 재순위화 절을 사용하여 결과를 재순위화합니다. 검색을 실행할 때 `rerank-search-pipeline`을 검색 파이프라인으로 사용합니다.\n",
    "\n",
    "`rerank_query_llm_with_rag` - 이 메서드는 위의 재순위화 검색 메서드를 호출하여 상위 청크를 검색한 다음, 이 상위 청크를 Claude Sonnet 3에 전달하여 사용자 질문에 답변합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ef7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_rerank_search(phrase, n, k, model_id=model_id, bedrock_client=bedrock_client):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"doc_chunk_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": k,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"doc_chunk_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{model_id}\",\n",
    "            \"k\": k\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "     \"ext\":{\n",
    "         \"rerank\": {\n",
    "          \"query_context\": {\n",
    "             \"query_text\": f\"{phrase}\"\n",
    "        }\n",
    "     }\n",
    "        \n",
    "    }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           search_pipeline=\"rerank-search-pipeline\",\n",
    "                           stored_fields=[\"doc_chunk_text\"],\n",
    "                           explain = False)\n",
    "    top_result = res['hits']['hits']\n",
    "\n",
    "    results = []\n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"id\":entry['_id'],\n",
    "            \"doc_chunk_text\":entry['_source']['doc_chunk_text'],\n",
    "            \"_score\":entry['_score']\n",
    "           \n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results[:n]\n",
    "\n",
    "\n",
    "def rerank_query_llm_with_rag(user_question, n, k):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_rerank_search(user_question, n = n, k = k, model_id=model_id, bedrock_client=bedrock_client)\n",
    "    system_prompt= \"You are a Financial report analysis bot analyzes provided text from financial documents and answers user questions\"\n",
    "    user_prompt = (\n",
    "        f\"As a financial report analyst please answer the question from the provided DOCS_DATA. If you cannot find answer from the DOCS_DATA, please say I'm sorry I cannot answer this question from given information. You do not have to mention that you got answer from DOCS_DATA if you got answer from DOCS_DATA.\\n\"\n",
    "        f\"Following is the DOCS_DATA after which you will be given the user's question to answer\\n\"\n",
    "        f\"DOCS_DATA: {retrieved_documents} \\n\"\n",
    "        f\"User's Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbac57",
   "metadata": {},
   "source": [
    "### 교차 인코더 검색 실행\n",
    "다음 셀은 이전에 답변하지 못했던 동일한 질문에 대해 검색을 실행합니다. K = 10개의 항목을 가져올 것이며, 이는 교차 인코더 모델에 의해 재순위화될 것입니다. 상위 K개 중에서 상위 N개의 레코드를 LLM에 전달하여 사용자의 질문에 답변하게 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56779f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_docs =\"What is AWS offering for retrieval augmented generation?\" # \"AWS는 검색 증강 생성(retrieval augmented generation)을 위해 어떤 서비스를 제공하고 있습니까?\"\n",
    "\n",
    "#keeping our results to top 3 items.\n",
    "n = 3\n",
    "recommendation = rerank_query_llm_with_rag(question_on_docs, n=3, k=10)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_rerank_search(phrase=question_on_docs, n=3, k=10), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4468ef",
   "metadata": {},
   "source": [
    "# 결론\n",
    "여러분은 Amazon 연간 재무 보고서를 PDF 파일 형식으로 로드하고 파싱하는 것으로 시작했습니다. 로드된 텍스트를 청크로 나누고 신경망 플러그인을 사용하여 직접 임베딩을 처리할 필요 없이 청크를 임베딩으로 변환했습니다. 그런 다음 의미론적 검색을 사용하여 이 데이터를 바탕으로 사용자의 금융 관련 질문에 답변했습니다. 강력한 키워드 검색과 새로운 의미론적 검색 기능을 결합하여 하이브리드 검색이 어떻게 검색 결과를 개선할 수 있는지 살펴보았습니다. 또한 재순위화가 RAG 파이프라인에서 중요한 개선 사항이 될 수 있음을 보여주었습니다.\n",
    "\n",
    "이 실습을 완료하신 것을 축하드립니다. 이제 실습 지침 섹션으로 이동하시면 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
