{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "[Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. \n",
    "\n",
    "In this lab we are going to be writing a RAG application code that allows user to ask questions about various wines so they can make a purchasing decision. We will use the semantic search (*vector search*) capability within OpenSearch to retrieve the best matching wine reviews and provide that to LLM for answering user's questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "#### a. Download and install python dependencies\n",
    "\n",
    "For this notebook we require a few libraries. We'll use the Python clients for OpenSearch and SageMaker, and OpenSearch ML Client library for generating text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd01d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.101 requires botocore==1.34.101, but you have botocore 1.34.143 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mPytorch version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install deprecated --quiet\n",
    "\n",
    "\n",
    "#OpenSearch Python SDK\n",
    "!pip install opensearch_py  --quiet\n",
    "#Progress bar for for loop\n",
    "!pip install alive-progress  --quiet\n",
    "\n",
    "# Let's import PyTorch and confirm that the latest version of PyTorch is running. \n",
    "# The version should already be 1.13.1 or higher. If not, we will restart the kernel.\n",
    "\n",
    "import torch\n",
    "pytorch_version = torch.__version__\n",
    "print( f\"Pytorch version: {pytorch_version}\")\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "if pytorch_version.startswith('1.1'):\n",
    "    from IPython.display import display_html\n",
    "    restartkernel()\n",
    "    \n",
    "##You may safely ignore pip dependencies errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "#### b. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459fff5",
   "metadata": {},
   "source": [
    "#### c. Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a CloudFormation stack in the account. Names and ARN of these resources will be used within this lab. We are going to load some of the information variables here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "## 3. Prepare data\n",
    "Below is the code that loads dataset of wine reviews, we'll use this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fc342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.json\"\n",
    "df = pd.read_json(url)\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "## 4. Create a connection with Amazon OpenSearch Service domain.\n",
    "Next, we'll use Python API to set up connection with OpenSearch domain.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "To avoid hard coding the user name and password in our code, we have dynamically generated a username and password at the time of deploying the cluster. This user name and password is stored in AWS Secrets Manager service. We will retrieve secret from Secrets Manager to establish OpenSearch connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "# For this lab we will use credentials that we have already created in AWS Secrets manager service. Secrets\n",
    "# manager service allows you to store secrets securily and retrieve it through code in a safe manner.\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe3d",
   "metadata": {},
   "source": [
    "## 5. Using Amazon Bedrock Titan Text embedding to convert text to vectors\n",
    "Amazon Bedrock service offers Amazon Titan Text embedding v2 model that generates vector embeddings for text. This model will be used as our primary model for embeddings.\n",
    "\n",
    "#### Helper method to invoke Titan Text embedding model in Amazon Bedrock\n",
    "Creating a helper method in python to invoke Amazon Titan Text v2 embedding model. We will update `df_sample` data frame and add a new column called `embedding` in it. Once this cell is executed, our data frame will be ready to load into OpenSearch. It may take a couple minutes to complete execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8cf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{region}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def add_embeddings_to_df(df, text_column):\n",
    "\n",
    "    # Create an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over the text in the specified column\n",
    "    for text in df[text_column]:\n",
    "        embedding = embed_phrase(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_phrase( text ):\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"  # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Extract the embedding from the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "\n",
    "    # Append the embedding to the list\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "df_sample = add_embeddings_to_df(df_sample, 'description')\n",
    "\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875df0b",
   "metadata": {},
   "source": [
    "#### Let's try to create an embedding of a simple input text\n",
    "You can see its an array of floating point numbers. While it does not make sense to human eye/brain, this array of numbers captures the semantics and knowledge of the text and that can be later used to compare two different text blocks. This method will be used to convert our query to a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43874004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create an vector embedding for input text\n",
    "input_text = \"A wine that pairs well with turkey breast?\"\n",
    "\n",
    "embedding = embed_phrase(input_text)\n",
    "\n",
    "#printing text and embedding\n",
    "\n",
    "print(f\"{input_text=}\")\n",
    "\n",
    "#only printing first 10 dimensions of the 1024 dimension vector \n",
    "print(f\"{embedding[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "## 7. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2-3 fields, this time we'll define the index with multiple fields: the vectorization of the `description` field, and all others present within the dataset.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"wine_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"wine_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "## 8. Load the raw data into the Index\n",
    "Next, let's load the wine review data and embedding into the index we've just created. Notice that we will store our embedding in `description_vector` field which will later be used for KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, record in tqdm(df_sample.iterrows()):\n",
    "    body={\"description_vector\": record['embedding'], \n",
    "           \"description\": record[\"description\"],\n",
    "           \"points\":record[\"points\"],\n",
    "           \"variety\":record[\"variety\"],\n",
    "           \"country\":record[\"country\"],\n",
    "           \"designation\":record[\"designation\"],\n",
    "           \"winery\":record[\"winery\"],\n",
    "          \"wine_name\":record[\"title\"]\n",
    "         }\n",
    "    aos_client.index(index=index_name, body=body)\n",
    "print(\"Records loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "## 9. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `retrieve_opensearch_with_semantic_search` embeds the search phrase, searches the index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    search_vector = embed_phrase(phrase)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"description_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"description\":entry['_source']['description'],\n",
    "            \"winery\":entry['_source']['winery'],\n",
    "            \"points\":entry['_source']['points'],\n",
    "            \"designation\":entry['_source']['designation'],\n",
    "            \"country\":entry['_source']['country'],\n",
    "            \"variety\":entry['_source']['variety'],\n",
    "            \"wine_name\":entry['_source']['wine_name'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_wine)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "## 10. Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with general purpose data, it may not have your wine review knowledge. While it may be able to answer, it may not be an answer that your business prefers. For example. in your case, you would not want it to recommend a wine that you do not stock. So the recommendation has to be one of the wines from your collection i.e. the 300 reviewed wines that we loaded. \n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions without the wine review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "    # Define the prompt for the model\n",
    "    prompt = \"Write a sonnet about the beauty of nature.\"\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Let's check the generated result for a wine recommendation. It may not be one of the wine that we stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "    \n",
    "    #Claude model has 2 parts of the prompt. System prompt guides the model what role to play\n",
    "    system_prompt = f\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    \n",
    "    #User prompt is the engineer prompt that has the context that model should reference to answer questions\n",
    "    user_prompt = (\n",
    "        f\" As a sommelier, you must include the wine variety, the country of origin, \"\n",
    "        f\"and a colorful description relating to the customer question.\"\n",
    "        f\"\\n Customer question: {question}\"\n",
    "        f\"\\n Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"\n",
    "    )\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "print(f\"The recommened wine from LLM without RAG: \\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c5a4c",
   "metadata": {},
   "source": [
    "#### Testing for hallucination. \n",
    "Let's copy the wine name from the last line and paste it in the question variable below to see if we have this wine in our stock. Please review the list of wines that are returned. They may be from portugal but not exactly the one we have been recommended by the model. That's called hallucination, as model came up with an arbitrary recommendation with its general purpose knowledge.\n",
    "\n",
    "__Note:__ If you do not see the wine name recommended by model above in the `wine_name` variable below, you should replace it so we can verify that the wine recommended is not in our opensearch index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff031f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine_name = \"Penfolds Bin 389 Kalimna Shiraz\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(wine_name)\n",
    "print(\"Matching wine records in our reviews:\")\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "## 11. Retrieval Augmented Generation\n",
    "---\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "#### Create a prompt for the LLM using the search results from OpenSearch (RAG)\n",
    "\n",
    "We will be using the Anthropic Claude Sonnet 3 model with one-shot prompting technique. Within instructions to the model in the prompt, we will provide a sample wine review and how model should use to answer user's question. At the end of the prompt wine reviews retrieved from Opensearch will be included for model to use. \n",
    "\n",
    "Before querying the model, the below function `generate_rag_based_system_prompt` is used to put together user prompt. The function takes in an input string to search the OpenSearch cluster for a matching wine, then compose the user prompt for LLM. \n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "**SYSTEM PROMPT:**\n",
    "\n",
    "```\n",
    "You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy. \n",
    "```\n",
    "\n",
    "\n",
    "**USER PROMPT**\n",
    "```\n",
    "As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user's question.\n",
    "\n",
    "Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Question from the user as is\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### package the prompt and query the LLM\n",
    "We will create a final function to query the LLM with the prompt. `query_llm_with_rag` is a function that calls LLM in a RAG.\n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- searches the OpenSearch index with semantic search for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(user_question)\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt= \"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy\"\n",
    "    user_prompt = (\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not suggest anything outside of the wine data provided. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Wine data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents} \\n\"\n",
    "        f\"Customer Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc562a1e",
   "metadata": {},
   "source": [
    "#### Let's change it to Italian wine - it should produce a matching result.\n",
    "We will call the same method again to see if there is an italian wine in our catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Italian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5fe7f",
   "metadata": {},
   "source": [
    "You might notice that we asked for Australian wines that goes well with steak and we do not have any such wine in our collection. Therefore the model politely excuses. You may change the question and see how LLM recommends a wine from our select list that best suites your question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab you built a simple wine recommendation chatbot. In this particular lab you used Amazon Bedrock titan v2 model to create vector embedding for our data. Then we loaded this data in OpenSearch index with `description_vector` field. At search time, we used Amazon Titan v2 model again to convert our query question into vector embedding and used semantic search to retrieve results. These results were then passed on to Anthropic Claude Sonnet 3 model which was able to recommend us a wine from within our catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09f4ad",
   "metadata": {},
   "source": [
    "## Lab finished - you may now go back to lab instructions section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
