{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d6de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG using RDS SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438f363-7fa7-4386-aa4f-3e0f5f168936",
   "metadata": {
    "tags": []
   },
   "source": [
    "Retrieval Augmented Generation is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process.\n",
    "\n",
    "In this lab we are going to be writing a simple RAG application code that allows user to ask questions about various wines so they can make a purchasing decision. We will use the semantic search (vector search) capability within RDS SQL Server to retrieve the best matching wine reviews and provide that to LLM for answering user's questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc457a05",
   "metadata": {},
   "source": [
    "### 1. Lab Pre-requisites\n",
    "a. Download and install python dependencies\n",
    "\n",
    "b. Download and install Microsoft Open Database Connectivity (ODBC) driver for SQL Server (Linux)\n",
    "\n",
    "For this notebook we require the use of a few libraries. We'll use the Python clients for SQL Server and SageMaker, and Python frameworks for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d704c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RHEL 7 and Oracle Linux 7\n",
    "! curl https://packages.microsoft.com/config/rhel/7/prod.repo | sudo tee /etc/yum.repos.d/mssql-release.repo\n",
    "! sudo yum remove unixODBC-utf16 unixODBC-utf16-devel #to avoid conflicts\n",
    "! sudo ACCEPT_EULA=Y yum install -y msodbcsql18\n",
    "\n",
    "# Optional: for bcp and sqlcmd\n",
    "! sudo ACCEPT_EULA=Y yum install -y mssql-tools18\n",
    "! echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n",
    "! source ~/.bashrc\n",
    "\n",
    "# For unixODBC development headers\n",
    "! sudo yum install -y unixODBC-devel\n",
    "\n",
    "# You can ignore pip dependency error thatâ€™s displayed towards the end of the installation process \n",
    "! pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.33.6\" \\\n",
    "    \"awscli>=1.31.6\" \\\n",
    "    \"botocore>=1.33.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c952d",
   "metadata": {},
   "source": [
    "### 2. Import libraries & initialize resource information\n",
    "\n",
    "The line below will import all the relevant libraries and modules used in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6ee4c-19cd-4135-9b27-68d0876bd9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fada6",
   "metadata": {},
   "source": [
    "### 3. Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a cloudformation stack in the account. Information of these resources will be used within this lab. We are going to load some of the information variables here.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de644eff-37b5-4d08-a7b4-ccdad668eae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "# Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "rds_host = outputs['RDSSQLServerEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5458225-ed56-46b7-b717-2ff17721187c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Prepare data\n",
    "Below is the code that loads dataset of wine reviews, we'll use this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52743be3-d407-4104-9f39-0f8d9536da95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/davestroud/Wine/master/winemag-data-130k-v2.json\"\n",
    "df = pd.read_json(url)\n",
    "columns = ['points', 'description', 'designation', 'variety', 'country', 'winery', 'title']\n",
    "df_sample = df[columns]\n",
    "df_sample = df_sample.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab799e8",
   "metadata": {},
   "source": [
    "Removing single quotes from the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643049e-5f6c-4432-af00-1ec376d933fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all single quotes from the following columns:\n",
    "df_sample['description'] = df_sample['description'].astype(str).str.replace(\"[']\", \"\", regex=True)\n",
    "df_sample['designation'] = df_sample['designation'].astype(str).str.replace(\"[']\", \"\", regex=True)\n",
    "df_sample['variety'] = df_sample['variety'].astype(str).str.replace(\"[']\", \"\", regex=True)\n",
    "df_sample['country'] = df_sample['country'].astype(str).str.replace(\"[']\", \"\", regex=True)\n",
    "df_sample['winery'] = df_sample['winery'].astype(str).str.replace(\"[']\", \"\", regex=True)\n",
    "df_sample['title'] = df_sample['title'].astype(str).str.replace(\"[']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf18ae2-3017-4f7f-aa86-0ddf3480361f",
   "metadata": {},
   "source": [
    "## 5. Using Amazon Bedrock Titan embedding to convert text to vectors\n",
    "Amazon Bedrock offers Amazon Titan embedding v2 model that generates vector embeddings for text. This model will be used as our primary model for embeddings.\n",
    "\n",
    "#### Helper method to invoke Titan embedding model in Amazon Bedrock\n",
    "Creating a helper method in python to invoke Amazon Titan embedding model from Amazon Bedrock to generate embeddings. We will update `df_sample` data frame and add a new column called `embedding` in it. Once this cell is executed, our data frame will be ready to load into opensearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33809b1-e68e-4acd-b613-8bd6b1dee839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{region}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def add_embeddings_to_df(df, text_column):\n",
    "\n",
    "    # Create an empty list to store embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate over the text in the specified column\n",
    "    for text in df[text_column]:\n",
    "        embedding = embed_phrase(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "\n",
    "    # Add the embeddings as a new column to the DataFrame\n",
    "    df['embedding'] = embeddings\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_phrase( text ):\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"  # \n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    # Prepare the request payload\n",
    "    request_payload = json.dumps({\"inputText\": text})\n",
    "\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=request_payload, modelId=model_id, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Extract the embedding from the response\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "\n",
    "    # Append the embedding to the list\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "df_sample = add_embeddings_to_df(df_sample, 'description')\n",
    "\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffbef7-c8a5-431d-b484-884343764be9",
   "metadata": {},
   "source": [
    "#### Let's try to create an embedding of a simple input text\n",
    "You can see its an array of floating point numbers. While it does not make sense to human eye/brain, this array of numbers captures the semantics and knowledge of the text and that can be later used to compare two different text blocks. This method will be used to convert our query to a vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd558-6296-4002-9bb3-3209af3c5b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create an vector embedding for input text\n",
    "input_text = \"A wine that pairs well with turkey breast?\"\n",
    "\n",
    "embedding = embed_phrase(input_text)\n",
    "\n",
    "#printing text and embedding\n",
    "\n",
    "print(f\"{input_text=}\")\n",
    "\n",
    "#only printing first 10 dimensions of the 1024 dimension vector \n",
    "print(f\"{embedding[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466229c0",
   "metadata": {},
   "source": [
    "### 6. Create a connection to RDS SQL Server\n",
    "\n",
    "Next, we'll use Python API to set up connection with RDS SQL Server instance, our vector database for this workshop.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "\n",
    "To avoid hard coding the user name and password in our code, we have dynamically generated a username and password at the time of deploying the cluster. This user name and password is stored in AWS Secrets Manager service. We will retrieve secret from secrets manager to establish database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333204fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "db_credentials = json.loads(kms.get_secret_value(SecretId=outputs['DBSecret'])['SecretString'])\n",
    "\n",
    "# For this lab we will use credentials that we have already created in AWS Secrets manager service. Secrets\n",
    "# manager service allows you to store secrets securily and retrieve it through code in a safe manner.\n",
    "\n",
    "dbuser = db_credentials['username']\n",
    "dbpass = db_credentials['password']\n",
    "print(dbuser,dbpass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e337a",
   "metadata": {},
   "source": [
    "#### Enable and Verify SageMaker Notebook instance to RDS connectivity\n",
    "\n",
    "Sagemaker instance and RDS SQL Server instance are deployed in different VPCs for the workshop. Enable the ingress rules to allow remote traffic and test the connectivity using sample python program below as part of preparation work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8346f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this to add the ingress rules to allow remote access to PostgreSQL\n",
    "!aws ec2 authorize-security-group-ingress --group-name default --protocol tcp --port 1433 --cidr 0.0.0.0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdea10e-c31a-43aa-ade0-d9188d69411f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "import pyodbc\n",
    "filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')\n",
    "\n",
    "try:\n",
    "    cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=master;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "    print (cnxn)\n",
    "    print (\"Test connection works.\")\n",
    "    cnxn.close()\n",
    "except Exception as e:\n",
    "    print(\"I am unable to connect to the database!\\n\")\n",
    "\n",
    "    print (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febdf05a",
   "metadata": {},
   "source": [
    "### 7. Preparing the raw data for insertion and the tables in RDS SQL Server\n",
    "\n",
    "In this step, we will prepare the raw data to be inserted into RDS. We will create the VectorDB and tables to store them. Final step would be to insert the data into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87ba8e-07b1-4a8c-8697-4aa4c7952f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map the Pandas Dataframe columns to the database table column names\n",
    "column_map = {\n",
    "    'index':'id',\n",
    "    'points': 'points',\n",
    "    'description': 'description',\n",
    "    'designation': 'designation',\n",
    "    'variety': 'variety',\n",
    "    'country': 'country',\n",
    "    'winery': 'winery',\n",
    "    'title': 'wine_name',\n",
    "    'record_id':'record_id',\n",
    "    'embedding': 'content_vector',\n",
    "}\n",
    "\n",
    "# Rename pandas dataframe columns\n",
    "df_sample.rename(columns=column_map, inplace=True)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aece0-3f03-440e-875c-da619d37039c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the VectorDB on RDS SQL Server\n",
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=master;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "cursor = cnxn.cursor()\n",
    "sql_stm = '''\n",
    "drop database if exists vectordb;\n",
    "create database vectordb;\n",
    "'''\n",
    "cnxn.autocommit = True\n",
    "cursor.execute(sql_stm)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27dc677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the target database table\n",
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "cursor = cnxn.cursor()\n",
    "sql_stm = '''\n",
    "drop table if exists dbo.wines_embedding_bedrock;\n",
    "CREATE TABLE [dbo].[wines_embedding_bedrock](\n",
    "    [id] [int],\n",
    "    [points] [int] NOT NULL,\n",
    "    [description] [varchar](max) NOT NULL,\n",
    "    [designation] [varchar](1000) NOT NULL,\n",
    "    [variety] [varchar](100) NOT NULL,\n",
    "    [country] [varchar](100) NOT NULL,\n",
    "    [winery] [varchar](100) NOT NULL,\n",
    "    [wine_name] [varchar](500) NOT NULL,\n",
    "    [record_id][int] NOT NULL,\n",
    "    [content_vector] [varchar](max) NOT NULL\n",
    ") ON [PRIMARY]\n",
    "'''\n",
    "cursor.execute(sql_stm)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb265f-c4a5-42a4-bfac-a329221f6adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for insertion\n",
    "data = df_sample.to_dict(orient='records')\n",
    "\n",
    "# Insert data into the database table.\n",
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "\n",
    "cursor = cnxn.cursor()\n",
    "sql_stm = '''TRUNCATE TABLE dbo.wines_embedding_bedrock'''\n",
    "cursor.execute(sql_stm)\n",
    "cnxn.commit()\n",
    "\n",
    "with cnxn.cursor() as cursor:\n",
    "    for row in data:\n",
    "        values = ', '.join(f\"'{value}'\" for value in row.values())\n",
    "        columns = ', '.join(row.keys())\n",
    "        query = f\"INSERT INTO wines_embedding_bedrock ({columns}) VALUES ({values})\"\n",
    "        # print(query)\n",
    "        cursor.execute(query)\n",
    "    cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe6f65",
   "metadata": {},
   "source": [
    "To verify that we have inserted 300 records into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff72b6f-f324-4042-9e0b-bc8328a21eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "sql_query = pd.read_sql(\"select count (1) as total_rows from wines_embedding_bedrock\", cnxn)\n",
    "df = pd.DataFrame(sql_query, columns=['total_rows'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec1d84",
   "metadata": {},
   "source": [
    "### 8. Create Vector table and Columnstore Index\n",
    "\n",
    "Next, we will create a vector table and a Columnstore Index. SQL Server Columnstore Index offer built-in optimizations including SIMD and AVX-512 that accelerate vector operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a67685-70cc-4b6b-8fc3-5ecb5d6f7415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Vector table and a columnstore index\n",
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "cursor = cnxn.cursor()\n",
    "sql_stm = '''\n",
    "drop table if exists dbo.wines_content_vector;\n",
    "with cte as\n",
    "(\n",
    "    select\n",
    "        v.id as index_id,\n",
    "        cast(tv.[key] as int) as vector_value_id,\n",
    "        cast(tv.[value] as float) as vector_value\n",
    "    from\n",
    "        [dbo].[wines_embedding_bedrock] as v\n",
    "    cross apply\n",
    "        openjson(content_vector) tv\n",
    ")\n",
    "select\n",
    "    index_id,\n",
    "    vector_value_id,\n",
    "    vector_value\n",
    "into\n",
    "    dbo.wines_content_vector\n",
    "from\n",
    "    cte;\n",
    "\n",
    "create clustered columnstore index ixc\n",
    "on dbo.wines_content_vector;\n",
    "'''\n",
    "#print (sql_stm)\n",
    "cursor.execute(sql_stm)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb1a0f",
   "metadata": {},
   "source": [
    "### 9. Create Similarity Search function\n",
    "\n",
    "Next, we create a user defined function (UDF) to support similarity search based on cosine distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c23bd-b677-49db-b429-17fb91ea38a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create UDF for Similarity search\n",
    "cnxn = pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no')\n",
    "cursor = cnxn.cursor()\n",
    "sql_stm = '''drop function if exists [dbo].[SimilarWines]'''\n",
    "sql_stm2 = '''\n",
    "create   function [dbo].[SimilarWines](@vector nvarchar(max))\n",
    "returns table\n",
    "as\n",
    "return with cteVector as\n",
    "(\n",
    "    select \n",
    "        cast([key] as int) as [vector_value_id],\n",
    "        cast([value] as float) as [vector_value]\n",
    "    from \n",
    "        openjson(@vector)\n",
    "),\n",
    "cteSimilar as\n",
    "(\n",
    "select top (50)\n",
    "    v2.index_id, \n",
    "    sum(v1.[vector_value] * v2.[vector_value]) as cosine_distance -- Optimized as per https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use\n",
    "from \n",
    "    cteVector v1\n",
    "inner join \n",
    "    dbo.wines_content_vector v2 on v1.vector_value_id = v2.vector_value_id\n",
    "group by\n",
    "    v2.index_id\n",
    "order by\n",
    "    cosine_distance desc\n",
    ")\n",
    "select \n",
    "    a.points,\n",
    "    a.description,\n",
    "    a.designation,\n",
    "    a.variety,\n",
    "    a.country,\n",
    "    a.winery,\n",
    "    a.wine_name,\n",
    "    r.cosine_distance\n",
    "from \n",
    "    cteSimilar r\n",
    "inner join \n",
    "    dbo.wines_embedding_bedrock a on r.index_id = a.id\n",
    "'''\n",
    "cursor.execute(sql_stm)\n",
    "cursor.execute(sql_stm2)\n",
    "cnxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867b3d4",
   "metadata": {},
   "source": [
    "\n",
    "### 10. Search vector with \"Semantic Search\"\n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. retrieve_sql_with_semantic_search embeds the search phrase, searches the table in RDS SQL Server for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229ccf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_sql_with_semantic_search(phrase):\n",
    "    n=3\n",
    "    search_vector = embed_phrase(phrase)\n",
    "    json_query_result = json.dumps(search_vector)\n",
    "\n",
    "    stmt = '''select top (?) points, description, designation, variety, country, winery, wine_name, cosine_distance \\\n",
    "    from dbo.SimilarWines(?) as r order by cosine_distance desc'''\n",
    "    results = []\n",
    "    \n",
    "    with pyodbc.connect( f'DRIVER=ODBC Driver 18 for SQL Server;SERVER={rds_host};DATABASE=vectordb;UID={dbuser};PWD={dbpass};Encrypt=no') as cnxn:\n",
    "        curs = cnxn.cursor()\n",
    "        curs.execute (stmt,(n,json_query_result))\n",
    "    \n",
    "        for record in curs:\n",
    "            result = {\n",
    "                \"points\":record[0],\n",
    "                \"description\":record[1],\n",
    "                \"designation\":record[2],\n",
    "                \"variety\":record[3],\n",
    "                \"country\":record[4],\n",
    "                \"winery\":record[5],\n",
    "                \"wine_name\":record[6]\n",
    "                }\n",
    "            results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b35d4",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe084a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "example_request = retrieve_sql_with_semantic_search(question_on_wine)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a8af8",
   "metadata": {},
   "source": [
    "### 11.  Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model\n",
    "\n",
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with static data, and it does not have our wine review knowledge. While it may be able to answer, it may not be an answer that a business prefers. For example. in our case, we would not want to recommend a wine that we do not stock. So the recommendation has to be one of the wines from our collection i.e. 300 reviews that we loaded.\n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions without the wine review data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20499a71-8b9b-4882-9dcc-871b9dcd2203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "    # Define the prompt for the model\n",
    "    prompt = \"Write a sonnet about the beauty of nature.\"\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6c81a",
   "metadata": {},
   "source": [
    "Let's check the generated result for a wine recommendation. It may not be one of the wine that we stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bd046-878a-46b3-89ef-25795de3ed64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "    \n",
    "    # Claude model has 2 parts of the prompt. System prompt guides the model what role to play\n",
    "    system_prompt = f\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    \n",
    "    # User prompt is the engineer prompt that has the context that model should reference to answer questions\n",
    "    user_prompt = (\n",
    "        f\" As a sommelier, you must include the wine variety, the country of origin, \"\n",
    "        f\"and a colorful description relating to the customer question.\"\n",
    "        f\"\\n Customer question: {question}\"\n",
    "        f\"\\n Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"\n",
    "    )\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "\n",
    "print(f\"The recommened wine from LLM without RAG: \\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee548d",
   "metadata": {},
   "source": [
    "#### Testing for hallucination.\n",
    "\n",
    "Let's copy the wine name from the last line and past it in the question variable below to see if we have this wine in our stock. Please review the list of wines that are returned. They may be from portugal but not exactly the one we have been recommended by the model.\n",
    "\n",
    "Note: If you do not see the same wine name in the wine_name variable below, you should replace it so we can verify that the wine recommended is not in our RDS SQL Server table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71978cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine_name = \"Penfolds Grange Shiraz\"\n",
    "example_request = retrieve_sql_with_semantic_search(wine_name)\n",
    "print(\"Matching wine records in our reviews:\")\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4566c",
   "metadata": {},
   "source": [
    "### 12. Retrieval Augmented Generation\n",
    "\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination.\n",
    "Create a prompt for the LLM using the search results from RDS SQL Server\n",
    "\n",
    "We will be using the Anthropic Sonnet model with one-shot prompting technique. Within instructions to the model in the prompt, we will provide a sample wine review and how model should use to answer user's question. At the end of the prompt wine reviews retrieved from RDS SQL Server will be included for model to use.\n",
    "\n",
    "Before querying the model, the below function generate_sql_based_system_prompt is used to put together user prompt. The function takes in an input string to search the wines_content_vector table for a matching wine, then compose the user prompt for LLM.\n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "#### SYSTEM PROMPT:\n",
    "\n",
    "    You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy. \n",
    "\n",
    "#### USER PROMPT\n",
    "\n",
    "    As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user's question.\n",
    "\n",
    "    Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "    Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "    Data: {retrieved_documents}\n",
    "\n",
    "    Question from the user as is\n",
    "\n",
    "#### Package the prompt and query the LLM\n",
    "\n",
    "We will create a final function to query the LLM with the prompt. query_llm_with_rag is a function that calls LLM in a RAG.\n",
    "\n",
    "query_llm_with_rag combines everything we've done in this module. It does all of the following:\n",
    "\n",
    "- searches the RDS SQL Server vector data with semantic search for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786ad17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_sql_with_semantic_search(user_question)\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt= \"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy\"\n",
    "    user_prompt = (\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not suggest anything outside of the wine data provided. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Wine data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents} \\n\"\n",
    "        f\"Customer Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b001bd",
   "metadata": {},
   "source": [
    "And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4c565-9c58-4e68-a743-ac503f850e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_sql_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3746a",
   "metadata": {},
   "source": [
    "Let's change it to Italian wine - it should produce a matching result.\n",
    "\n",
    "We will call the same method again to see if there is an italian wine in our catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32152ba1-9b91-4ca7-9fdb-33e3bf7c7d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Italian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_sql_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678aad6",
   "metadata": {},
   "source": [
    "You might notice that we asked for Australian wines that goes well with steak and we do not have any such wine in our collection. Therefore the model politely excuses. You may change the question and see how LLM recommends a wine from our select list that best suites your question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e17f7",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab you built a simple wine recommendation chatbot. In this particular lab you used Amazon Bedrock titan v2 model to create vector embedding for our data. Then we loaded this data in OpenSearch index with `description_vector` field. At search time, we used Amazon Titan v2 model again to convert our query question into vector embedding and used semantic search to retrieve results. These results were then passed on to Anthropic Claude Sonnet 3 model which was able to recommend us a wine from within our catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796fcfc",
   "metadata": {},
   "source": [
    "## Lab finished - you may now go back to lab instructions section"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
